{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/mayureshagashe2105/Convolutional-Neural-Networks-CNN-from-scratch/blob/main/TFOD_API_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VyzZDW_wsAhp"
   },
   "source": [
    "# TensorFlow Object Detetcion API setup tutorial\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "In this end-to-end tutorial, we will install the TFOD API and will train a custom object detection model.\n",
    "\n",
    "For this tutorial we will follow [Docs](https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/) with few additional steps, thereby resolving all dependency issues!\n",
    "\n",
    "## Content\n",
    "- [] Cloning tensorflow/model repository\n",
    "- [] Installing protbuf & cocoapi\n",
    "- [] Defining proper folder structure\n",
    "- [] Installing TFOD API\n",
    "- [] Instaling pre-trained-model\n",
    "- [] Creating xml files of Images via labelimg\n",
    "- [] Converting xml files to tfrecords and creating label map\n",
    "- [] Training the model\n",
    "- [] Exporting the model\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3cHOSzhnu6-z"
   },
   "source": [
    "# 1. Cloning tensorflow/model repository\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "djyuNFn_LJ_y",
    "outputId": "e6d96a9e-c383-43cc-e0be-d236fbcc56e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'models'...\n",
      "remote: Enumerating objects: 60342, done.\u001b[K\n",
      "remote: Counting objects: 100% (108/108), done.\u001b[K\n",
      "remote: Compressing objects: 100% (101/101), done.\u001b[K\n",
      "remote: Total 60342 (delta 44), reused 69 (delta 5), pack-reused 60234\u001b[K\n",
      "Receiving objects: 100% (60342/60342), 573.85 MiB | 26.01 MiB/s, done.\n",
      "Resolving deltas: 100% (41946/41946), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/tensorflow/models.git # cloning repo to colab storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OX22IUy8LWz7",
    "outputId": "89b81c86-b021-4850-f122-e73716afefab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/models/research\n"
     ]
    }
   ],
   "source": [
    "cd /content/models/research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "wwS7XswcLaxG",
    "outputId": "0c90b3f0-1a49-473e-98b8-7d5c128eb426"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/content/models/research'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd # make sure you have the same output!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0lWuBGnjwbm9"
   },
   "source": [
    "## Checklist\n",
    "- [x] Cloning tensorflow/model repository\n",
    "- [] Installing protbuf & cocoapi\n",
    "- [] Defining proper folder structure\n",
    "- [] Installing TFOD API\n",
    "- [] Instaling pre-trained-model\n",
    "- [] Creating xml files of Images via labelimg\n",
    "- [] Converting xml files to tfrecords and creating label map\n",
    "- [] Training the model\n",
    "- [] Exporting the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y531h2jtvjVs"
   },
   "source": [
    "# 2. Installing protbuf & cocoapi\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "IZZ01QaPLcYt"
   },
   "outputs": [],
   "source": [
    "!protoc object_detection/protos/*.proto --python_out=. # protbuf installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yKIgNhLoLgBd",
    "outputId": "2b397d6b-30ca-4aa4-ddba-2e2f7e9550b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'cocoapi'...\n",
      "remote: Enumerating objects: 975, done.\u001b[K\n",
      "remote: Total 975 (delta 0), reused 0 (delta 0), pack-reused 975\u001b[K\n",
      "Receiving objects: 100% (975/975), 11.72 MiB | 2.05 MiB/s, done.\n",
      "Resolving deltas: 100% (576/576), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/cocodataset/cocoapi.git # clonning cocoapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H_pD5kunLgct",
    "outputId": "eeef187c-7b88-4e8a-e9c3-67ec7b98ce6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/models/research/cocoapi/PythonAPI\n"
     ]
    }
   ],
   "source": [
    "cd cocoapi/PythonAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GbACRVYTLh9j",
    "outputId": "491de024-c342-4fb9-86b3-41b3427be1f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python setup.py build_ext --inplace\n",
      "running build_ext\n",
      "skipping 'pycocotools/_mask.c' Cython extension (up-to-date)\n",
      "building 'pycocotools._mask' extension\n",
      "creating build\n",
      "creating build/common\n",
      "creating build/temp.linux-x86_64-3.7\n",
      "creating build/temp.linux-x86_64-3.7/pycocotools\n",
      "x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fdebug-prefix-map=/build/python3.7-LSlbJj/python3.7-3.7.11=. -fstack-protector-strong -Wformat -Werror=format-security -g -fdebug-prefix-map=/build/python3.7-LSlbJj/python3.7-3.7.11=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.7/dist-packages/numpy/core/include -I../common -I/usr/include/python3.7m -c ../common/maskApi.c -o build/temp.linux-x86_64-3.7/../common/maskApi.o -Wno-cpp -Wno-unused-function -std=c99\n",
      "\u001b[01m\u001b[K../common/maskApi.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KrleDecode\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[K../common/maskApi.c:46:7:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K-Wmisleading-indentation\u001b[m\u001b[K]\n",
      "       \u001b[01;35m\u001b[Kfor\u001b[m\u001b[K( k=0; k<R[i].cnts[j]; k++ ) *(M++)=v; v=!v; }}\n",
      "       \u001b[01;35m\u001b[K^~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K../common/maskApi.c:46:49:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’\n",
      "       for( k=0; k<R[i].cnts[j]; k++ ) *(M++)=v; \u001b[01;36m\u001b[Kv\u001b[m\u001b[K=!v; }}\n",
      "                                                 \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K../common/maskApi.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KrleFrPoly\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[K../common/maskApi.c:166:3:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K-Wmisleading-indentation\u001b[m\u001b[K]\n",
      "   \u001b[01;35m\u001b[Kfor\u001b[m\u001b[K(j=0; j<k; j++) x[j]=(int)(scale*xy[j*2+0]+.5); x[k]=x[0];\n",
      "   \u001b[01;35m\u001b[K^~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K../common/maskApi.c:166:54:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’\n",
      "   for(j=0; j<k; j++) x[j]=(int)(scale*xy[j*2+0]+.5); \u001b[01;36m\u001b[Kx\u001b[m\u001b[K[k]=x[0];\n",
      "                                                      \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K../common/maskApi.c:167:3:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K-Wmisleading-indentation\u001b[m\u001b[K]\n",
      "   \u001b[01;35m\u001b[Kfor\u001b[m\u001b[K(j=0; j<k; j++) y[j]=(int)(scale*xy[j*2+1]+.5); y[k]=y[0];\n",
      "   \u001b[01;35m\u001b[K^~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K../common/maskApi.c:167:54:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’\n",
      "   for(j=0; j<k; j++) y[j]=(int)(scale*xy[j*2+1]+.5); \u001b[01;36m\u001b[Ky\u001b[m\u001b[K[k]=y[0];\n",
      "                                                      \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K../common/maskApi.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KrleToString\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[K../common/maskApi.c:212:7:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kif\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K-Wmisleading-indentation\u001b[m\u001b[K]\n",
      "       \u001b[01;35m\u001b[Kif\u001b[m\u001b[K(more) c |= 0x20; c+=48; s[p++]=c;\n",
      "       \u001b[01;35m\u001b[K^~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K../common/maskApi.c:212:27:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kif\u001b[m\u001b[K’\n",
      "       if(more) c |= 0x20; \u001b[01;36m\u001b[Kc\u001b[m\u001b[K+=48; s[p++]=c;\n",
      "                           \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K../common/maskApi.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KrleFrString\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[K../common/maskApi.c:220:3:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kwhile\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K-Wmisleading-indentation\u001b[m\u001b[K]\n",
      "   \u001b[01;35m\u001b[Kwhile\u001b[m\u001b[K( s[m] ) m++; cnts=malloc(sizeof(uint)*m); m=0;\n",
      "   \u001b[01;35m\u001b[K^~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K../common/maskApi.c:220:22:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kwhile\u001b[m\u001b[K’\n",
      "   while( s[m] ) m++; \u001b[01;36m\u001b[Kcnts\u001b[m\u001b[K=malloc(sizeof(uint)*m); m=0;\n",
      "                      \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K../common/maskApi.c:228:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kif\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K-Wmisleading-indentation\u001b[m\u001b[K]\n",
      "     \u001b[01;35m\u001b[Kif\u001b[m\u001b[K(m>2) x+=(long) cnts[m-2]; cnts[m++]=(uint) x;\n",
      "     \u001b[01;35m\u001b[K^~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K../common/maskApi.c:228:34:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kif\u001b[m\u001b[K’\n",
      "     if(m>2) x+=(long) cnts[m-2]; \u001b[01;36m\u001b[Kcnts\u001b[m\u001b[K[m++]=(uint) x;\n",
      "                                  \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K../common/maskApi.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KrleToBbox\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[K../common/maskApi.c:141:31:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kxp\u001b[m\u001b[K’ may be used uninitialized in this function [\u001b[01;35m\u001b[K-Wmaybe-uninitialized\u001b[m\u001b[K]\n",
      "       if(j%2==0) xp=x; else if\u001b[01;35m\u001b[K(\u001b[m\u001b[Kxp<x) { ys=0; ye=h-1; }\n",
      "                               \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
      "x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fdebug-prefix-map=/build/python3.7-LSlbJj/python3.7-3.7.11=. -fstack-protector-strong -Wformat -Werror=format-security -g -fdebug-prefix-map=/build/python3.7-LSlbJj/python3.7-3.7.11=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.7/dist-packages/numpy/core/include -I../common -I/usr/include/python3.7m -c pycocotools/_mask.c -o build/temp.linux-x86_64-3.7/pycocotools/_mask.o -Wno-cpp -Wno-unused-function -std=c99\n",
      "creating build/lib.linux-x86_64-3.7\n",
      "creating build/lib.linux-x86_64-3.7/pycocotools\n",
      "x86_64-linux-gnu-gcc -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fdebug-prefix-map=/build/python3.7-LSlbJj/python3.7-3.7.11=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.7/../common/maskApi.o build/temp.linux-x86_64-3.7/pycocotools/_mask.o -o build/lib.linux-x86_64-3.7/pycocotools/_mask.cpython-37m-x86_64-linux-gnu.so\n",
      "copying build/lib.linux-x86_64-3.7/pycocotools/_mask.cpython-37m-x86_64-linux-gnu.so -> pycocotools\n",
      "rm -rf build\n"
     ]
    }
   ],
   "source": [
    "!make # As colab is a linux machine at backend, we are using thihs command to compile cocoapi's wrapper PythonAPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XBo-uGlFxHEb"
   },
   "source": [
    "copy `pycocotools` to /content/models/research .\n",
    "This is required for training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "CfUzI3wCLjDY"
   },
   "outputs": [],
   "source": [
    "cp -r pycocotools /content/models/research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BMI5LFTTxgfE"
   },
   "source": [
    "## Checklist\n",
    "- [x] Cloning tensorflow/model repository\n",
    "- [x] Installing protbuf & cocoapi\n",
    "- [] Defining proper folder structure\n",
    "- [] Installing TFOD API\n",
    "- [] Instaling pre-trained-model\n",
    "- [] Creating xml files of Images via labelimg\n",
    "- [] Converting xml files to tfrecords and creating label map\n",
    "- [] Training the model\n",
    "- [] Exporting the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ku0E5ykpLsGO"
   },
   "source": [
    "# 3. Defining proper folder structure\n",
    "\n",
    "---\n",
    "Defining proper folder structure will help us to keep our code files and data separate and will be easy to navigate through! This is also recommended in the official Documents! Now Let's try to understand the folder structure. First, we will go to the `content` directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "c3mf6eJeJDxa"
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "81RdmDX7ME1F",
    "outputId": "b18ae582-9eae-44ea-d985-724a616d4e83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n"
     ]
    }
   ],
   "source": [
    "cd /content/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "sm_H0kHLMK_B"
   },
   "outputs": [],
   "source": [
    "PATHS = {'ANNOTATIONS': 'annotations', 'IMAGES': 'images', \n",
    "         'MODELS': 'models', 'PRE-TRAINED_MODELS': 'pre-trained-models', 'EXPORTED_MODELS': 'exported_models'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fJwDHDWyEmYR"
   },
   "source": [
    "## Understanding the folder structure\n",
    "Folder | Purpose |\n",
    ":-: | :-: \n",
    "training_demo | This is the main directory which will contain subfolders and scripts|\n",
    "annotations | This folder will contain the label map for our custom object detection model and tfrecord files for training |\n",
    "images | Holds images for annotation |\n",
    "models | Folder for storing our trained model |\n",
    "pre-trained-models | Contains pre-trained model which we will download from model zoo |\n",
    "exported_models | Holds our trained, custom object detection model, which can be reused |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BTCUHeWaNAXL",
    "outputId": "7cc25eee-9af1-4a25-ac72-1c8d2783a773"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully created /content/training_demo\n",
      "successfully created /content/training_demo/annotations\n",
      "successfully created /content/training_demo/images\n",
      "successfully created /content/training_demo/models\n",
      "successfully created /content/training_demo/pre-trained-models\n",
      "successfully created /content/training_demo/exported_models\n",
      "successfully created /content/training_demo/images/train\n",
      "successfully created /content/training_demo/images/test\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('/content/training_demo'):\n",
    "  DIR = '/content'\n",
    "  WORKSPACE = 'training_demo'\n",
    "  DIR = os.path.join(DIR, WORKSPACE)\n",
    "  !mkdir {DIR}\n",
    "  print(f'successfully created {DIR}')\n",
    "  for key in PATHS:\n",
    "    SUB_DIR = os.path.join(DIR, PATHS[key])\n",
    "    !mkdir {SUB_DIR}\n",
    "    print(f'successfully created {SUB_DIR}')\n",
    "  \n",
    "  SUB_DIR = os.path.join(DIR, PATHS['IMAGES'], 'train')\n",
    "  !mkdir {SUB_DIR}\n",
    "  print(f'successfully created {SUB_DIR}')\n",
    "\n",
    "  SUB_DIR = os.path.join(DIR, PATHS['IMAGES'], 'test')\n",
    "  !mkdir {SUB_DIR}\n",
    "  print(f'successfully created {SUB_DIR}')\n",
    "\n",
    "\n",
    "else:\n",
    "  print('Directory already exists!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ks9hngdJILc"
   },
   "source": [
    "### Now the folder structure should look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YQZKQmuAJSCq",
    "outputId": "f5cb0a29-51d7-4033-a4df-291a1a6176b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "The following package was automatically installed and is no longer required:\n",
      "  libnvidia-common-460\n",
      "Use 'sudo apt autoremove' to remove it.\n",
      "The following NEW packages will be installed:\n",
      "  tree\n",
      "0 upgraded, 1 newly installed, 0 to remove and 40 not upgraded.\n",
      "Need to get 40.7 kB of archives.\n",
      "After this operation, 105 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tree amd64 1.7.0-5 [40.7 kB]\n",
      "Fetched 40.7 kB in 1s (53.6 kB/s)\n",
      "debconf: unable to initialize frontend: Dialog\n",
      "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
      "debconf: falling back to frontend: Readline\n",
      "debconf: unable to initialize frontend: Readline\n",
      "debconf: (This frontend requires a controlling tty.)\n",
      "debconf: falling back to frontend: Teletype\n",
      "dpkg-preconfigure: unable to re-open stdin: \n",
      "Selecting previously unselected package tree.\n",
      "(Reading database ... 148486 files and directories currently installed.)\n",
      "Preparing to unpack .../tree_1.7.0-5_amd64.deb ...\n",
      "Unpacking tree (1.7.0-5) ...\n",
      "Setting up tree (1.7.0-5) ...\n",
      "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"
     ]
    }
   ],
   "source": [
    "!sudo apt-get install tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mY3KHSppJWu0",
    "outputId": "7b3566bd-ac8f-4acd-f018-5d475b35c1af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/training_demo\n",
      "├── annotations\n",
      "├── exported_models\n",
      "├── images\n",
      "│   ├── test\n",
      "│   └── train\n",
      "├── models\n",
      "└── pre-trained-models\n",
      "\n",
      "7 directories, 0 files\n"
     ]
    }
   ],
   "source": [
    "!tree /content/training_demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ht15Yq2MJzZr"
   },
   "source": [
    "## Checklist\n",
    "- [x] Cloning tensorflow/model repository\n",
    "- [x] Installing protbuf & cocoapi\n",
    "- [x] Defining proper folder structure\n",
    "- [] Installing TFOD API\n",
    "- [] Instaling pre-trained-model\n",
    "- [] Creating xml files of Images via labelimg\n",
    "- [] Converting xml files to tfrecords and creating label map\n",
    "- [] Training the model\n",
    "- [] Exporting the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MQLEsEH1J18j"
   },
   "source": [
    "# Installing TFOD API\n",
    "\n",
    "---\n",
    "This step is rather important and is likely to cause dependency issues, so please follow the instructions neatly!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UAJ4Ll90OOGd",
    "outputId": "97d64da7-04a0-4ee5-abf2-d742b4000494"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/models/research\n"
     ]
    }
   ],
   "source": [
    "cd /content/models/research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aElKTOn3KN3y"
   },
   "source": [
    "Copy the `object_detection/packages/tf2/setup.py` from /content/models/research to root directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "IBzVrsf7R7gq"
   },
   "outputs": [],
   "source": [
    "cp object_detection/packages/tf2/setup.py ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5xxMVxdpLCkD"
   },
   "source": [
    "First, we will install tensorflow 2.5.0 if a different version is in use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "NetqcymQSPl_",
    "outputId": "880ad2c7-5349-4816-fb34-23afda3e716b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==2.5.0\n",
      "  Downloading tensorflow-2.5.0-cp37-cp37m-manylinux2010_x86_64.whl (454.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 454.3 MB 17 kB/s \n",
      "\u001b[?25hRequirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (1.6.3)\n",
      "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (0.37.0)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (1.12)\n",
      "Requirement already satisfied: tensorboard~=2.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (2.6.0)\n",
      "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (1.19.5)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (1.1.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (3.17.3)\n",
      "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (1.15.0)\n",
      "Collecting tensorflow-estimator<2.6.0,>=2.5.0rc0\n",
      "  Downloading tensorflow_estimator-2.5.0-py2.py3-none-any.whl (462 kB)\n",
      "\u001b[K     |████████████████████████████████| 462 kB 42.4 MB/s \n",
      "\u001b[?25hRequirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (3.1.0)\n",
      "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (0.4.0)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (0.2.0)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (3.7.4.3)\n",
      "Collecting keras-nightly~=2.5.0.dev\n",
      "  Downloading keras_nightly-2.5.0.dev2021032900-py2.py3-none-any.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 41.5 MB/s \n",
      "\u001b[?25hRequirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (3.3.0)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (1.12.1)\n",
      "Collecting grpcio~=1.34.0\n",
      "  Downloading grpcio-1.34.1-cp37-cp37m-manylinux2014_x86_64.whl (4.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.0 MB 47.6 MB/s \n",
      "\u001b[?25hRequirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (1.1.2)\n",
      "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (0.12.0)\n",
      "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow==2.5.0) (1.5.2)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0) (1.8.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0) (57.4.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0) (1.34.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0) (0.6.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0) (2.23.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0) (0.4.5)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0) (1.0.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0) (3.3.4)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.0) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.0) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.0) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow==2.5.0) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow==2.5.0) (4.6.4)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.0) (0.4.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.0) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.0) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.0) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.0) (2021.5.30)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow==2.5.0) (3.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.5->tensorflow==2.5.0) (3.5.0)\n",
      "Installing collected packages: grpcio, tensorflow-estimator, keras-nightly, tensorflow\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.39.0\n",
      "    Uninstalling grpcio-1.39.0:\n",
      "      Successfully uninstalled grpcio-1.39.0\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.6.0\n",
      "    Uninstalling tensorflow-estimator-2.6.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.6.0\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.6.0\n",
      "    Uninstalling tensorflow-2.6.0:\n",
      "      Successfully uninstalled tensorflow-2.6.0\n",
      "Successfully installed grpcio-1.34.1 keras-nightly-2.5.0.dev2021032900 tensorflow-2.5.0 tensorflow-estimator-2.5.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "tensorflow"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "if tf.__version__ != '2.5.0':\n",
    "  !pip install tensorflow==2.5.0\n",
    "else:\n",
    "  print('requirement already satisfied!!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2HKzxn3-SvTq"
   },
   "source": [
    "# Now Colab should propmt you to Restart the runtime!!!\n",
    "\n",
    "Click on `RESTART RUNTIME` and then run the cells with the following code. Do not run all the cells!!!\n",
    "\n",
    "Cells to re-run:\n",
    "1. `cd /content/models/research` Link: \n",
    " [LINK TO THE CELL](#scrollTo=OX22IUy8LWz7&line=1&uniqifier=1)\n",
    "2. `cd cocoapi/PythonAPI` Link: [LINK TO THE CELL](#scrollTo=H_pD5kunLgct&line=1&uniqifier=1)\n",
    "3. `!make` Link: [LINK TO THE CELL](#scrollTo=GbACRVYTLh9j&line=1&uniqifier=1)\n",
    "4. `cp -r pycocotools /content/models/research` Link: [LINK TO THE CELL](#scrollTo=CfUzI3wCLjDY&line=1&uniqifier=1)\n",
    "5. `import os` Link: [LINK TO THE CELL](#scrollTo=c3mf6eJeJDxa&line=1&uniqifier=1)\n",
    "6.  `cd /content/models/research` Link: [LINK TO THE CELL](#scrollTo=UAJ4Ll90OOGd&line=1&uniqifier=1)\n",
    "7. `cp object_detection/packages/tf2/setup.py .` Link: [LINK TO THE CELL](#scrollTo=IBzVrsf7R7gq&line=1&uniqifier=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gPmTWfW5QhRj"
   },
   "source": [
    "  ### Now run the code below\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BLw_Ih8vSKkr",
    "outputId": "873b9635-906e-460d-e75e-d81f1ebef69d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: --use-feature=2020-resolver no longer has any effect, since it is now the default dependency resolver in pip. This will become an error in pip 21.0.\u001b[0m\n",
      "Processing /content/models/research\n",
      "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
      "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
      "Collecting avro-python3\n",
      "  Downloading avro-python3-1.10.2.tar.gz (38 kB)\n",
      "Collecting apache-beam\n",
      "  Downloading apache_beam-2.31.0-cp37-cp37m-manylinux2010_x86_64.whl (9.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 9.7 MB 7.2 MB/s \n",
      "\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (7.1.2)\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (4.2.6)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (3.2.2)\n",
      "Requirement already satisfied: Cython in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (0.29.24)\n",
      "Requirement already satisfied: contextlib2 in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (0.5.5)\n",
      "Collecting tf-slim\n",
      "  Downloading tf_slim-1.1.0-py2.py3-none-any.whl (352 kB)\n",
      "\u001b[K     |████████████████████████████████| 352 kB 32.1 MB/s \n",
      "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (1.15.0)\n",
      "Requirement already satisfied: pycocotools in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (2.0.2)\n",
      "Collecting lvis\n",
      "  Downloading lvis-0.5.3-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (1.4.1)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (1.1.5)\n",
      "Collecting tf-models-official>=2.5.1\n",
      "  Downloading tf_models_official-2.6.0-py2.py3-none-any.whl (1.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.8 MB 36.6 MB/s \n",
      "\u001b[?25hCollecting tensorflow-model-optimization>=0.4.1\n",
      "  Downloading tensorflow_model_optimization-0.6.0-py2.py3-none-any.whl (211 kB)\n",
      "\u001b[K     |████████████████████████████████| 211 kB 36.2 MB/s \n",
      "\u001b[?25hCollecting pyyaml>=5.1\n",
      "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
      "\u001b[K     |████████████████████████████████| 636 kB 21.9 MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (1.19.5)\n",
      "Requirement already satisfied: gin-config in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (0.4.0)\n",
      "Collecting opencv-python-headless\n",
      "  Downloading opencv_python_headless-4.5.3.56-cp37-cp37m-manylinux2014_x86_64.whl (37.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 37.1 MB 46 kB/s \n",
      "\u001b[?25hRequirement already satisfied: tensorflow-hub>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (0.12.0)\n",
      "Collecting tensorflow-text>=2.5.0\n",
      "  Downloading tensorflow_text-2.6.0-cp37-cp37m-manylinux1_x86_64.whl (4.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.4 MB 40.3 MB/s \n",
      "\u001b[?25hRequirement already satisfied: psutil>=5.4.3 in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (5.4.8)\n",
      "Requirement already satisfied: kaggle>=1.3.9 in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (1.5.12)\n",
      "Requirement already satisfied: tensorflow>=2.5.0 in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (2.5.0)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 38.1 MB/s \n",
      "\u001b[?25hCollecting sacrebleu\n",
      "  Downloading sacrebleu-2.0.0-py3-none-any.whl (90 kB)\n",
      "\u001b[K     |████████████████████████████████| 90 kB 9.4 MB/s \n",
      "\u001b[?25hCollecting seqeval\n",
      "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
      "\u001b[K     |████████████████████████████████| 43 kB 1.8 MB/s \n",
      "\u001b[?25hRequirement already satisfied: google-api-python-client>=1.6.7 in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (1.12.8)\n",
      "Requirement already satisfied: oauth2client in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (4.1.3)\n",
      "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (4.0.1)\n",
      "Collecting tensorflow-addons\n",
      "  Downloading tensorflow_addons-0.13.0-cp37-cp37m-manylinux2010_x86_64.whl (679 kB)\n",
      "\u001b[K     |████████████████████████████████| 679 kB 46.3 MB/s \n",
      "\u001b[?25hCollecting py-cpuinfo>=3.3.0\n",
      "  Downloading py-cpuinfo-8.0.0.tar.gz (99 kB)\n",
      "\u001b[K     |████████████████████████████████| 99 kB 9.2 MB/s \n",
      "\u001b[?25hRequirement already satisfied: google-auth>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (1.34.0)\n",
      "Requirement already satisfied: google-api-core<2dev,>=1.21.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (1.26.3)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (0.0.4)\n",
      "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (3.0.1)\n",
      "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (0.17.4)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (1.53.0)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (3.17.3)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (2.23.0)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (57.4.0)\n",
      "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (21.0)\n",
      "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (2018.9)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.16.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.16.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.16.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (0.2.8)\n",
      "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (2.8.2)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (2021.5.30)\n",
      "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (5.0.2)\n",
      "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (1.24.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (4.62.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (2.4.7)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.16.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (0.4.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (3.0.4)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (1.12.1)\n",
      "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (0.4.0)\n",
      "Requirement already satisfied: keras-nightly~=2.5.0.dev in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (2.5.0.dev2021032900)\n",
      "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (3.1.0)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (1.1.0)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (1.1.2)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (1.6.3)\n",
      "Requirement already satisfied: grpcio~=1.34.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (1.34.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (2.5.0)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (1.12)\n",
      "Requirement already satisfied: tensorboard~=2.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (2.6.0)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (3.7.4.3)\n",
      "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (0.12.0)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (3.3.0)\n",
      "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (0.37.0)\n",
      "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (1.5.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (1.8.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (0.4.5)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (1.0.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (3.3.4)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (4.6.4)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (3.1.1)\n",
      "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-model-optimization>=0.4.1->tf-models-official>=2.5.1->object-detection==0.1) (0.1.6)\n",
      "Collecting tensorflow>=2.5.0\n",
      "  Downloading tensorflow-2.6.0-cp37-cp37m-manylinux2010_x86_64.whl (458.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 458.3 MB 11 kB/s \n",
      "\u001b[?25hRequirement already satisfied: keras~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (2.6.0)\n",
      "INFO: pip is looking at multiple versions of tensorflow-text to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting tensorflow-text>=2.5.0\n",
      "  Downloading tensorflow_text-2.5.0-cp37-cp37m-manylinux1_x86_64.whl (4.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.3 MB 36.2 MB/s \n",
      "\u001b[?25hCollecting future<1.0.0,>=0.18.2\n",
      "  Downloading future-0.18.2.tar.gz (829 kB)\n",
      "\u001b[K     |████████████████████████████████| 829 kB 47.5 MB/s \n",
      "\u001b[?25hCollecting hdfs<3.0.0,>=2.1.0\n",
      "  Downloading hdfs-2.6.0-py3-none-any.whl (33 kB)\n",
      "Requirement already satisfied: pyarrow<5.0.0,>=0.15.1 in /usr/local/lib/python3.7/dist-packages (from apache-beam->object-detection==0.1) (3.0.0)\n",
      "Collecting dill<0.3.2,>=0.3.1.1\n",
      "  Downloading dill-0.3.1.1.tar.gz (151 kB)\n",
      "\u001b[K     |████████████████████████████████| 151 kB 43.7 MB/s \n",
      "\u001b[?25hRequirement already satisfied: pymongo<4.0.0,>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam->object-detection==0.1) (3.12.0)\n",
      "Collecting requests<3.0.0dev,>=2.18.0\n",
      "  Downloading requests-2.26.0-py2.py3-none-any.whl (62 kB)\n",
      "\u001b[K     |████████████████████████████████| 62 kB 757 kB/s \n",
      "\u001b[?25hRequirement already satisfied: crcmod<2.0,>=1.7 in /usr/local/lib/python3.7/dist-packages (from apache-beam->object-detection==0.1) (1.7)\n",
      "Requirement already satisfied: pydot<2,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam->object-detection==0.1) (1.3.0)\n",
      "Collecting avro-python3\n",
      "  Downloading avro-python3-1.9.2.1.tar.gz (37 kB)\n",
      "Collecting fastavro<2,>=0.21.4\n",
      "  Downloading fastavro-1.4.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.3 MB 37.0 MB/s \n",
      "\u001b[?25hRequirement already satisfied: docopt in /usr/local/lib/python3.7/dist-packages (from hdfs<3.0.0,>=2.1.0->apache-beam->object-detection==0.1) (0.6.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (2.0.4)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.5->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (3.5.0)\n",
      "Requirement already satisfied: kiwisolver>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from lvis->object-detection==0.1) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from lvis->object-detection==0.1) (0.10.0)\n",
      "Requirement already satisfied: opencv-python>=4.1.0.25 in /usr/local/lib/python3.7/dist-packages (from lvis->object-detection==0.1) (4.1.2.30)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (1.3)\n",
      "Collecting colorama\n",
      "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from sacrebleu->tf-models-official>=2.5.1->object-detection==0.1) (2019.12.20)\n",
      "Collecting portalocker\n",
      "  Downloading portalocker-2.3.0-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu->tf-models-official>=2.5.1->object-detection==0.1) (0.8.9)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval->tf-models-official>=2.5.1->object-detection==0.1) (0.22.2.post1)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official>=2.5.1->object-detection==0.1) (1.0.1)\n",
      "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons->tf-models-official>=2.5.1->object-detection==0.1) (2.7.1)\n",
      "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (21.2.0)\n",
      "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (5.2.2)\n",
      "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (1.2.0)\n",
      "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (2.3)\n",
      "Building wheels for collected packages: object-detection, py-cpuinfo, avro-python3, dill, future, seqeval\n",
      "  Building wheel for object-detection (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for object-detection: filename=object_detection-0.1-py3-none-any.whl size=1660787 sha256=c9dc45ea35751d47cc5e780220a9a9365c64ed0866aec21b84cf14cbdfb84e56\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-nr8nx7rl/wheels/fa/a4/d2/e9a5057e414fd46c8e543d2706cd836d64e1fcd9eccceb2329\n",
      "  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for py-cpuinfo: filename=py_cpuinfo-8.0.0-py3-none-any.whl size=22258 sha256=766cde7f6e4129e33cf190718db0f1988f20f8263a14b0996883630d3bcb6c38\n",
      "  Stored in directory: /root/.cache/pip/wheels/d2/f1/1f/041add21dc9c4220157f1bd2bd6afe1f1a49524c3396b94401\n",
      "  Building wheel for avro-python3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for avro-python3: filename=avro_python3-1.9.2.1-py3-none-any.whl size=43512 sha256=d219b78e6335c047c363eea272e640b3a1f236dae7055cf2bd176f37a63c61ce\n",
      "  Stored in directory: /root/.cache/pip/wheels/bc/49/5f/fdb5b9d85055c478213e0158ac122b596816149a02d82e0ab1\n",
      "  Building wheel for dill (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for dill: filename=dill-0.3.1.1-py3-none-any.whl size=78546 sha256=1597fe691360121b6d74d81ae27603b791c8246da643655ea88b79b3f168da42\n",
      "  Stored in directory: /root/.cache/pip/wheels/a4/61/fd/c57e374e580aa78a45ed78d5859b3a44436af17e22ca53284f\n",
      "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=84e3abff6b78cf0b0c8913a8b0e306819f1e4ba622fddfc03d402c81c8b6159c\n",
      "  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
      "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16181 sha256=6bef993ed62ff312f900599b93049a18e1dbcf751cd63c062abd9738384e5562\n",
      "  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\n",
      "Successfully built object-detection py-cpuinfo avro-python3 dill future seqeval\n",
      "Installing collected packages: requests, portalocker, future, dill, colorama, tf-slim, tensorflow-text, tensorflow-model-optimization, tensorflow-addons, seqeval, sentencepiece, sacrebleu, pyyaml, py-cpuinfo, opencv-python-headless, hdfs, fastavro, avro-python3, tf-models-official, lvis, apache-beam, object-detection\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.23.0\n",
      "    Uninstalling requests-2.23.0:\n",
      "      Successfully uninstalled requests-2.23.0\n",
      "  Attempting uninstall: future\n",
      "    Found existing installation: future 0.16.0\n",
      "    Uninstalling future-0.16.0:\n",
      "      Successfully uninstalled future-0.16.0\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.4\n",
      "    Uninstalling dill-0.3.4:\n",
      "      Successfully uninstalled dill-0.3.4\n",
      "  Attempting uninstall: pyyaml\n",
      "    Found existing installation: PyYAML 3.13\n",
      "    Uninstalling PyYAML-3.13:\n",
      "      Successfully uninstalled PyYAML-3.13\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "multiprocess 0.70.12.2 requires dill>=0.3.4, but you have dill 0.3.1.1 which is incompatible.\n",
      "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.26.0 which is incompatible.\n",
      "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
      "Successfully installed apache-beam-2.31.0 avro-python3-1.9.2.1 colorama-0.4.4 dill-0.3.1.1 fastavro-1.4.4 future-0.18.2 hdfs-2.6.0 lvis-0.5.3 object-detection-0.1 opencv-python-headless-4.5.3.56 portalocker-2.3.0 py-cpuinfo-8.0.0 pyyaml-5.4.1 requests-2.26.0 sacrebleu-2.0.0 sentencepiece-0.1.96 seqeval-1.2.2 tensorflow-addons-0.13.0 tensorflow-model-optimization-0.6.0 tensorflow-text-2.5.0 tf-models-official-2.6.0 tf-slim-1.1.0\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install --use-feature=2020-resolver ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9KcC1jaQTXOd"
   },
   "source": [
    "Now as we have used-package resolver for installing all dependencies, we will now install tensorflow's version with which the model is compatible. In this case, we will be using `SSD_ResNet101_V1_FPN` so we will install its compatible tf version which is `2.2.0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "HmYweewLSM6z",
    "outputId": "bc4498e3-492d-49b8-abef-12ae15eaeec8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==2.2.0\n",
      "  Downloading tensorflow-2.2.0-cp37-cp37m-manylinux2010_x86_64.whl (516.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 516.2 MB 3.9 kB/s \n",
      "\u001b[?25hCollecting h5py<2.11.0,>=2.10.0\n",
      "  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9 MB 41.1 MB/s \n",
      "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (0.37.0)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (3.17.3)\n",
      "Collecting tensorflow-estimator<2.3.0,>=2.2.0\n",
      "  Downloading tensorflow_estimator-2.2.0-py2.py3-none-any.whl (454 kB)\n",
      "\u001b[K     |████████████████████████████████| 454 kB 40.4 MB/s \n",
      "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.15.0)\n",
      "Collecting tensorboard<2.3.0,>=2.2.0\n",
      "  Downloading tensorboard-2.2.2-py3-none-any.whl (3.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0 MB 42.3 MB/s \n",
      "\u001b[?25hCollecting gast==0.3.3\n",
      "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.19.5)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.34.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (3.3.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.1.2)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (0.2.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.1.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.12.1)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (0.12.0)\n",
      "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.4.1)\n",
      "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.6.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2.26.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.8.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (57.4.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.34.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.4.5)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.3.4)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.0.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (4.6.4)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.4.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2021.5.30)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.5.0)\n",
      "Installing collected packages: tensorflow-estimator, tensorboard, h5py, gast, tensorflow\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.5.0\n",
      "    Uninstalling tensorflow-estimator-2.5.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.5.0\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.6.0\n",
      "    Uninstalling tensorboard-2.6.0:\n",
      "      Successfully uninstalled tensorboard-2.6.0\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 3.1.0\n",
      "    Uninstalling h5py-3.1.0:\n",
      "      Successfully uninstalled h5py-3.1.0\n",
      "  Attempting uninstall: gast\n",
      "    Found existing installation: gast 0.4.0\n",
      "    Uninstalling gast-0.4.0:\n",
      "      Successfully uninstalled gast-0.4.0\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.5.0\n",
      "    Uninstalling tensorflow-2.5.0:\n",
      "      Successfully uninstalled tensorflow-2.5.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tf-models-official 2.6.0 requires tensorflow>=2.5.0, but you have tensorflow 2.2.0 which is incompatible.\n",
      "tensorflow-text 2.5.0 requires tensorflow<2.6,>=2.5.0, but you have tensorflow 2.2.0 which is incompatible.\u001b[0m\n",
      "Successfully installed gast-0.3.3 h5py-2.10.0 tensorboard-2.2.2 tensorflow-2.2.0 tensorflow-estimator-2.2.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "gast",
         "h5py",
         "tensorboard",
         "tensorflow"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "if tf.__version__ != '2.2.0':\n",
    "  !pip install tensorflow==2.2.0\n",
    "else:\n",
    "  print('requirement already satisfied!!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rbs5-bwkYh_j"
   },
   "source": [
    "# Now Colab should propmt you to Restart the runtime!!!\n",
    "\n",
    "Click on `RESTART RUNTIME` and then run the cells with the following code. Do not run all the cells!!!\n",
    "\n",
    "Cells to re-run:\n",
    "1. `cd /content/models/research` Link: \n",
    " [LINK TO THE CELL](#scrollTo=OX22IUy8LWz7&line=1&uniqifier=1)\n",
    "2. `cd cocoapi/PythonAPI` Link: [LINK TO THE CELL](#scrollTo=H_pD5kunLgct&line=1&uniqifier=1)\n",
    "3. `!make` Link: [LINK TO THE CELL](#scrollTo=GbACRVYTLh9j&line=1&uniqifier=1)\n",
    "4. `cp -r pycocotools /content/models/research` Link: [LINK TO THE CELL](#scrollTo=CfUzI3wCLjDY&line=1&uniqifier=1)\n",
    "5. `import os` Link: [LINK TO THE CELL](#scrollTo=c3mf6eJeJDxa&line=1&uniqifier=1)\n",
    "6.  `cd /content/models/research` Link: [LINK TO THE CELL](#scrollTo=UAJ4Ll90OOGd&line=1&uniqifier=1)\n",
    "7. `cp object_detection/packages/tf2/setup.py .` Link: [LINK TO THE CELL](#scrollTo=IBzVrsf7R7gq&line=1&uniqifier=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JjA9bA-9UYT2",
    "outputId": "e98757d8-85f9-4c9a-ded6-7ffaf2e904b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hgGu35t1Y3Ox"
   },
   "source": [
    "As keras engine is a higher level API it is version specific for tf 2.2.0 we need tensorflow-addons 0.10.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cdh1kFNLTdMX",
    "outputId": "72ca670d-8932-4ccd-bcff-a2ba403bd460"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-addons==0.10.0\n",
      "  Downloading tensorflow_addons-0.10.0-cp37-cp37m-manylinux2010_x86_64.whl (1.0 MB)\n",
      "\u001b[?25l\r",
      "\u001b[K     |▎                               | 10 kB 22.2 MB/s eta 0:00:01\r",
      "\u001b[K     |▋                               | 20 kB 10.4 MB/s eta 0:00:01\r",
      "\u001b[K     |█                               | 30 kB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |█▎                              | 40 kB 7.8 MB/s eta 0:00:01\r",
      "\u001b[K     |█▋                              | 51 kB 4.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█▉                              | 61 kB 4.4 MB/s eta 0:00:01\r",
      "\u001b[K     |██▏                             | 71 kB 4.7 MB/s eta 0:00:01\r",
      "\u001b[K     |██▌                             | 81 kB 5.3 MB/s eta 0:00:01\r",
      "\u001b[K     |██▉                             | 92 kB 3.9 MB/s eta 0:00:01\r",
      "\u001b[K     |███▏                            | 102 kB 4.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███▌                            | 112 kB 4.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███▊                            | 122 kB 4.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████                            | 133 kB 4.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████▍                           | 143 kB 4.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████▊                           | 153 kB 4.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█████                           | 163 kB 4.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█████▎                          | 174 kB 4.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█████▋                          | 184 kB 4.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████                          | 194 kB 4.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████▎                         | 204 kB 4.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████▋                         | 215 kB 4.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████                         | 225 kB 4.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████▏                        | 235 kB 4.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████▌                        | 245 kB 4.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████▉                        | 256 kB 4.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████▏                       | 266 kB 4.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████▌                       | 276 kB 4.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████▊                       | 286 kB 4.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████                       | 296 kB 4.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▍                      | 307 kB 4.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▊                      | 317 kB 4.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████                      | 327 kB 4.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▍                     | 337 kB 4.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▋                     | 348 kB 4.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████                     | 358 kB 4.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▎                    | 368 kB 4.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▋                    | 378 kB 4.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████                    | 389 kB 4.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▎                   | 399 kB 4.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▌                   | 409 kB 4.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▉                   | 419 kB 4.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▏                  | 430 kB 4.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▌                  | 440 kB 4.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▉                  | 450 kB 4.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████                  | 460 kB 4.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▍                 | 471 kB 4.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▊                 | 481 kB 4.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████                 | 491 kB 4.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▍                | 501 kB 4.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▊                | 512 kB 4.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████                | 522 kB 4.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▎               | 532 kB 4.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▋               | 542 kB 4.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████               | 552 kB 4.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▎              | 563 kB 4.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▌              | 573 kB 4.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▉              | 583 kB 4.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▏             | 593 kB 4.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▌             | 604 kB 4.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▉             | 614 kB 4.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▏            | 624 kB 4.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▍            | 634 kB 4.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▊            | 645 kB 4.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████            | 655 kB 4.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▍           | 665 kB 4.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▊           | 675 kB 4.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████           | 686 kB 4.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▎          | 696 kB 4.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▋          | 706 kB 4.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████          | 716 kB 4.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▎         | 727 kB 4.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▋         | 737 kB 4.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▉         | 747 kB 4.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▏        | 757 kB 4.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▌        | 768 kB 4.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▉        | 778 kB 4.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▏       | 788 kB 4.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▌       | 798 kB 4.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▊       | 808 kB 4.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████       | 819 kB 4.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▍      | 829 kB 4.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▊      | 839 kB 4.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████      | 849 kB 4.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▎     | 860 kB 4.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▋     | 870 kB 4.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████     | 880 kB 4.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▎    | 890 kB 4.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▋    | 901 kB 4.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████    | 911 kB 4.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▏   | 921 kB 4.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▌   | 931 kB 4.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▉   | 942 kB 4.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▏  | 952 kB 4.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▌  | 962 kB 4.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▉  | 972 kB 4.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████  | 983 kB 4.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▍ | 993 kB 4.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▊ | 1.0 MB 4.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████ | 1.0 MB 4.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▍| 1.0 MB 4.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▋| 1.0 MB 4.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 1.0 MB 4.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 1.0 MB 4.2 MB/s \n",
      "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons==0.10.0) (2.7.1)\n",
      "Installing collected packages: tensorflow-addons\n",
      "  Attempting uninstall: tensorflow-addons\n",
      "    Found existing installation: tensorflow-addons 0.13.0\n",
      "    Uninstalling tensorflow-addons-0.13.0:\n",
      "      Successfully uninstalled tensorflow-addons-0.13.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tf-models-official 2.6.0 requires tensorflow>=2.5.0, but you have tensorflow 2.2.0 which is incompatible.\u001b[0m\n",
      "Successfully installed tensorflow-addons-0.10.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow-addons==0.10.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ohTYTZKSZN7M"
   },
   "source": [
    "## This is the verification script that tells whether or not TFOD API is installed\n",
    "\n",
    "\n",
    "---\n",
    "After running the below cell, at the end if you see :\n",
    "`OK (skipped=1)` , congrats!!! we have successfully installed TFOD API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Uj49w2bSTguW",
    "outputId": "483799ee-de10-4b65-8f67-36edfd8a30b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running tests under Python 3.7.11: /usr/bin/python3\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_center_net_deepmac\n",
      "W0819 05:59:11.527511 140352138418048 model_builder.py:1088] Building experimental DeepMAC meta-arch. Some features may be omitted.\n",
      "[       OK ] ModelBuilderTF2Test.test_create_center_net_deepmac\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_center_net_model0 (customize_head_params=True)\n",
      "[       OK ] ModelBuilderTF2Test.test_create_center_net_model0 (customize_head_params=True)\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_center_net_model1 (customize_head_params=False)\n",
      "[       OK ] ModelBuilderTF2Test.test_create_center_net_model1 (customize_head_params=False)\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_center_net_model_from_keypoints\n",
      "[       OK ] ModelBuilderTF2Test.test_create_center_net_model_from_keypoints\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_center_net_model_mobilenet\n",
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "W0819 05:59:13.654281 140352138418048 mobilenet_v2.py:280] `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "2021-08-19 05:59:13.658579: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
      "2021-08-19 05:59:13.722489: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-08-19 05:59:13.723353: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n",
      "pciBusID: 0000:00:04.0 name: Tesla K80 computeCapability: 3.7\n",
      "coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s\n",
      "2021-08-19 05:59:13.737041: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2021-08-19 05:59:13.962364: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "2021-08-19 05:59:14.089146: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
      "2021-08-19 05:59:14.111326: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
      "2021-08-19 05:59:14.404497: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
      "2021-08-19 05:59:14.438723: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
      "2021-08-19 05:59:14.957493: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2021-08-19 05:59:14.957820: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-08-19 05:59:14.958723: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-08-19 05:59:14.959450: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n",
      "2021-08-19 05:59:14.961466: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2021-08-19 05:59:14.967880: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2299995000 Hz\n",
      "2021-08-19 05:59:14.968136: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5607ff76aa00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-08-19 05:59:14.968175: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-08-19 05:59:15.118460: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-08-19 05:59:15.119365: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5607ff76a840 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2021-08-19 05:59:15.119400: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
      "2021-08-19 05:59:15.120571: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-08-19 05:59:15.121356: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n",
      "pciBusID: 0000:00:04.0 name: Tesla K80 computeCapability: 3.7\n",
      "coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s\n",
      "2021-08-19 05:59:15.121436: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2021-08-19 05:59:15.121487: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "2021-08-19 05:59:15.121528: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
      "2021-08-19 05:59:15.121578: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
      "2021-08-19 05:59:15.121617: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
      "2021-08-19 05:59:15.121655: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
      "2021-08-19 05:59:15.121774: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2021-08-19 05:59:15.121913: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-08-19 05:59:15.122699: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-08-19 05:59:15.123404: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n",
      "2021-08-19 05:59:15.123477: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2021-08-19 05:59:15.125131: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-08-19 05:59:15.125166: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \n",
      "2021-08-19 05:59:15.125190: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N \n",
      "2021-08-19 05:59:15.125348: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-08-19 05:59:15.126100: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-08-19 05:59:15.126814: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2021-08-19 05:59:15.126866: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10699 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
      "9412608/9406464 [==============================] - 0s 0us/step\n",
      "[       OK ] ModelBuilderTF2Test.test_create_center_net_model_mobilenet\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_experimental_model\n",
      "[       OK ] ModelBuilderTF2Test.test_create_experimental_model\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)\n",
      "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)\n",
      "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner\n",
      "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul\n",
      "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul\n",
      "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul\n",
      "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul\n",
      "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_rfcn_model_from_config\n",
      "[       OK ] ModelBuilderTF2Test.test_create_rfcn_model_from_config\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config\n",
      "[       OK ] ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_ssd_models_from_config\n",
      "I0819 05:59:24.261914 140352138418048 ssd_efficientnet_bifpn_feature_extractor.py:143] EfficientDet EfficientNet backbone version: efficientnet-b0\n",
      "I0819 05:59:24.262193 140352138418048 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet BiFPN num filters: 64\n",
      "I0819 05:59:24.262352 140352138418048 ssd_efficientnet_bifpn_feature_extractor.py:146] EfficientDet BiFPN num iterations: 3\n",
      "I0819 05:59:24.272382 140352138418048 efficientnet_model.py:147] round_filter input=32 output=32\n",
      "I0819 05:59:24.328958 140352138418048 efficientnet_model.py:147] round_filter input=32 output=32\n",
      "I0819 05:59:24.329200 140352138418048 efficientnet_model.py:147] round_filter input=16 output=16\n",
      "I0819 05:59:24.483231 140352138418048 efficientnet_model.py:147] round_filter input=16 output=16\n",
      "I0819 05:59:24.483455 140352138418048 efficientnet_model.py:147] round_filter input=24 output=24\n",
      "I0819 05:59:25.062340 140352138418048 efficientnet_model.py:147] round_filter input=24 output=24\n",
      "I0819 05:59:25.062549 140352138418048 efficientnet_model.py:147] round_filter input=40 output=40\n",
      "I0819 05:59:25.485403 140352138418048 efficientnet_model.py:147] round_filter input=40 output=40\n",
      "I0819 05:59:25.485616 140352138418048 efficientnet_model.py:147] round_filter input=80 output=80\n",
      "I0819 05:59:26.141022 140352138418048 efficientnet_model.py:147] round_filter input=80 output=80\n",
      "I0819 05:59:26.141271 140352138418048 efficientnet_model.py:147] round_filter input=112 output=112\n",
      "I0819 05:59:26.772146 140352138418048 efficientnet_model.py:147] round_filter input=112 output=112\n",
      "I0819 05:59:26.772371 140352138418048 efficientnet_model.py:147] round_filter input=192 output=192\n",
      "I0819 05:59:27.647728 140352138418048 efficientnet_model.py:147] round_filter input=192 output=192\n",
      "I0819 05:59:27.647978 140352138418048 efficientnet_model.py:147] round_filter input=320 output=320\n",
      "I0819 05:59:27.844744 140352138418048 efficientnet_model.py:147] round_filter input=1280 output=1280\n",
      "I0819 05:59:27.938833 140352138418048 efficientnet_model.py:458] Building model efficientnet with params ModelConfig(width_coefficient=1.0, depth_coefficient=1.0, resolution=224, dropout_rate=0.2, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
      "I0819 05:59:28.040616 140352138418048 ssd_efficientnet_bifpn_feature_extractor.py:143] EfficientDet EfficientNet backbone version: efficientnet-b1\n",
      "I0819 05:59:28.040846 140352138418048 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet BiFPN num filters: 88\n",
      "I0819 05:59:28.040954 140352138418048 ssd_efficientnet_bifpn_feature_extractor.py:146] EfficientDet BiFPN num iterations: 4\n",
      "I0819 05:59:28.047821 140352138418048 efficientnet_model.py:147] round_filter input=32 output=32\n",
      "I0819 05:59:28.098836 140352138418048 efficientnet_model.py:147] round_filter input=32 output=32\n",
      "I0819 05:59:28.099095 140352138418048 efficientnet_model.py:147] round_filter input=16 output=16\n",
      "I0819 05:59:28.420236 140352138418048 efficientnet_model.py:147] round_filter input=16 output=16\n",
      "I0819 05:59:28.420457 140352138418048 efficientnet_model.py:147] round_filter input=24 output=24\n",
      "I0819 05:59:29.262540 140352138418048 efficientnet_model.py:147] round_filter input=24 output=24\n",
      "I0819 05:59:29.262794 140352138418048 efficientnet_model.py:147] round_filter input=40 output=40\n",
      "I0819 05:59:29.919273 140352138418048 efficientnet_model.py:147] round_filter input=40 output=40\n",
      "I0819 05:59:29.919549 140352138418048 efficientnet_model.py:147] round_filter input=80 output=80\n",
      "I0819 05:59:30.783158 140352138418048 efficientnet_model.py:147] round_filter input=80 output=80\n",
      "I0819 05:59:30.783409 140352138418048 efficientnet_model.py:147] round_filter input=112 output=112\n",
      "I0819 05:59:31.676644 140352138418048 efficientnet_model.py:147] round_filter input=112 output=112\n",
      "I0819 05:59:31.676897 140352138418048 efficientnet_model.py:147] round_filter input=192 output=192\n",
      "I0819 05:59:32.793015 140352138418048 efficientnet_model.py:147] round_filter input=192 output=192\n",
      "I0819 05:59:32.793250 140352138418048 efficientnet_model.py:147] round_filter input=320 output=320\n",
      "I0819 05:59:33.227897 140352138418048 efficientnet_model.py:147] round_filter input=1280 output=1280\n",
      "I0819 05:59:33.317185 140352138418048 efficientnet_model.py:458] Building model efficientnet with params ModelConfig(width_coefficient=1.0, depth_coefficient=1.1, resolution=240, dropout_rate=0.2, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
      "I0819 05:59:33.444224 140352138418048 ssd_efficientnet_bifpn_feature_extractor.py:143] EfficientDet EfficientNet backbone version: efficientnet-b2\n",
      "I0819 05:59:33.444473 140352138418048 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet BiFPN num filters: 112\n",
      "I0819 05:59:33.444582 140352138418048 ssd_efficientnet_bifpn_feature_extractor.py:146] EfficientDet BiFPN num iterations: 5\n",
      "I0819 05:59:33.451993 140352138418048 efficientnet_model.py:147] round_filter input=32 output=32\n",
      "I0819 05:59:33.511995 140352138418048 efficientnet_model.py:147] round_filter input=32 output=32\n",
      "I0819 05:59:33.512248 140352138418048 efficientnet_model.py:147] round_filter input=16 output=16\n",
      "I0819 05:59:33.839382 140352138418048 efficientnet_model.py:147] round_filter input=16 output=16\n",
      "I0819 05:59:33.839596 140352138418048 efficientnet_model.py:147] round_filter input=24 output=24\n",
      "I0819 05:59:34.519516 140352138418048 efficientnet_model.py:147] round_filter input=24 output=24\n",
      "I0819 05:59:34.519769 140352138418048 efficientnet_model.py:147] round_filter input=40 output=48\n",
      "I0819 05:59:35.419734 140352138418048 efficientnet_model.py:147] round_filter input=40 output=48\n",
      "I0819 05:59:35.419958 140352138418048 efficientnet_model.py:147] round_filter input=80 output=88\n",
      "I0819 05:59:36.309955 140352138418048 efficientnet_model.py:147] round_filter input=80 output=88\n",
      "I0819 05:59:36.310174 140352138418048 efficientnet_model.py:147] round_filter input=112 output=120\n",
      "I0819 05:59:37.199840 140352138418048 efficientnet_model.py:147] round_filter input=112 output=120\n",
      "I0819 05:59:37.200058 140352138418048 efficientnet_model.py:147] round_filter input=192 output=208\n",
      "I0819 05:59:38.350183 140352138418048 efficientnet_model.py:147] round_filter input=192 output=208\n",
      "I0819 05:59:38.350437 140352138418048 efficientnet_model.py:147] round_filter input=320 output=352\n",
      "I0819 05:59:38.781231 140352138418048 efficientnet_model.py:147] round_filter input=1280 output=1408\n",
      "I0819 05:59:38.864998 140352138418048 efficientnet_model.py:458] Building model efficientnet with params ModelConfig(width_coefficient=1.1, depth_coefficient=1.2, resolution=260, dropout_rate=0.3, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
      "I0819 05:59:38.995923 140352138418048 ssd_efficientnet_bifpn_feature_extractor.py:143] EfficientDet EfficientNet backbone version: efficientnet-b3\n",
      "I0819 05:59:38.996212 140352138418048 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet BiFPN num filters: 160\n",
      "I0819 05:59:38.996345 140352138418048 ssd_efficientnet_bifpn_feature_extractor.py:146] EfficientDet BiFPN num iterations: 6\n",
      "I0819 05:59:39.005606 140352138418048 efficientnet_model.py:147] round_filter input=32 output=40\n",
      "I0819 05:59:39.067552 140352138418048 efficientnet_model.py:147] round_filter input=32 output=40\n",
      "I0819 05:59:39.067800 140352138418048 efficientnet_model.py:147] round_filter input=16 output=24\n",
      "I0819 05:59:39.400559 140352138418048 efficientnet_model.py:147] round_filter input=16 output=24\n",
      "I0819 05:59:39.400801 140352138418048 efficientnet_model.py:147] round_filter input=24 output=32\n",
      "I0819 05:59:40.075208 140352138418048 efficientnet_model.py:147] round_filter input=24 output=32\n",
      "I0819 05:59:40.075440 140352138418048 efficientnet_model.py:147] round_filter input=40 output=48\n",
      "I0819 05:59:40.745306 140352138418048 efficientnet_model.py:147] round_filter input=40 output=48\n",
      "I0819 05:59:40.745514 140352138418048 efficientnet_model.py:147] round_filter input=80 output=96\n",
      "I0819 05:59:42.168651 140352138418048 efficientnet_model.py:147] round_filter input=80 output=96\n",
      "I0819 05:59:42.168917 140352138418048 efficientnet_model.py:147] round_filter input=112 output=136\n",
      "I0819 05:59:43.307571 140352138418048 efficientnet_model.py:147] round_filter input=112 output=136\n",
      "I0819 05:59:43.307849 140352138418048 efficientnet_model.py:147] round_filter input=192 output=232\n",
      "I0819 05:59:44.660992 140352138418048 efficientnet_model.py:147] round_filter input=192 output=232\n",
      "I0819 05:59:44.661252 140352138418048 efficientnet_model.py:147] round_filter input=320 output=384\n",
      "I0819 05:59:45.090464 140352138418048 efficientnet_model.py:147] round_filter input=1280 output=1536\n",
      "I0819 05:59:45.184198 140352138418048 efficientnet_model.py:458] Building model efficientnet with params ModelConfig(width_coefficient=1.2, depth_coefficient=1.4, resolution=300, dropout_rate=0.3, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
      "I0819 05:59:45.320581 140352138418048 ssd_efficientnet_bifpn_feature_extractor.py:143] EfficientDet EfficientNet backbone version: efficientnet-b4\n",
      "I0819 05:59:45.320830 140352138418048 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet BiFPN num filters: 224\n",
      "I0819 05:59:45.320944 140352138418048 ssd_efficientnet_bifpn_feature_extractor.py:146] EfficientDet BiFPN num iterations: 7\n",
      "I0819 05:59:45.332135 140352138418048 efficientnet_model.py:147] round_filter input=32 output=48\n",
      "I0819 05:59:45.386953 140352138418048 efficientnet_model.py:147] round_filter input=32 output=48\n",
      "I0819 05:59:45.387180 140352138418048 efficientnet_model.py:147] round_filter input=16 output=24\n",
      "I0819 05:59:45.713734 140352138418048 efficientnet_model.py:147] round_filter input=16 output=24\n",
      "I0819 05:59:45.713976 140352138418048 efficientnet_model.py:147] round_filter input=24 output=32\n",
      "I0819 05:59:46.624124 140352138418048 efficientnet_model.py:147] round_filter input=24 output=32\n",
      "I0819 05:59:46.624360 140352138418048 efficientnet_model.py:147] round_filter input=40 output=56\n",
      "I0819 05:59:47.547240 140352138418048 efficientnet_model.py:147] round_filter input=40 output=56\n",
      "I0819 05:59:47.547489 140352138418048 efficientnet_model.py:147] round_filter input=80 output=112\n",
      "I0819 05:59:48.943767 140352138418048 efficientnet_model.py:147] round_filter input=80 output=112\n",
      "I0819 05:59:48.943996 140352138418048 efficientnet_model.py:147] round_filter input=112 output=160\n",
      "I0819 05:59:50.323662 140352138418048 efficientnet_model.py:147] round_filter input=112 output=160\n",
      "I0819 05:59:50.323917 140352138418048 efficientnet_model.py:147] round_filter input=192 output=272\n",
      "I0819 05:59:52.555995 140352138418048 efficientnet_model.py:147] round_filter input=192 output=272\n",
      "I0819 05:59:52.556300 140352138418048 efficientnet_model.py:147] round_filter input=320 output=448\n",
      "I0819 05:59:52.993324 140352138418048 efficientnet_model.py:147] round_filter input=1280 output=1792\n",
      "I0819 05:59:53.086616 140352138418048 efficientnet_model.py:458] Building model efficientnet with params ModelConfig(width_coefficient=1.4, depth_coefficient=1.8, resolution=380, dropout_rate=0.4, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
      "I0819 05:59:53.246047 140352138418048 ssd_efficientnet_bifpn_feature_extractor.py:143] EfficientDet EfficientNet backbone version: efficientnet-b5\n",
      "I0819 05:59:53.246267 140352138418048 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet BiFPN num filters: 288\n",
      "I0819 05:59:53.246394 140352138418048 ssd_efficientnet_bifpn_feature_extractor.py:146] EfficientDet BiFPN num iterations: 7\n",
      "I0819 05:59:53.253471 140352138418048 efficientnet_model.py:147] round_filter input=32 output=48\n",
      "I0819 05:59:53.320713 140352138418048 efficientnet_model.py:147] round_filter input=32 output=48\n",
      "I0819 05:59:53.320936 140352138418048 efficientnet_model.py:147] round_filter input=16 output=24\n",
      "I0819 05:59:53.848372 140352138418048 efficientnet_model.py:147] round_filter input=16 output=24\n",
      "I0819 05:59:53.848577 140352138418048 efficientnet_model.py:147] round_filter input=24 output=40\n",
      "I0819 05:59:55.003903 140352138418048 efficientnet_model.py:147] round_filter input=24 output=40\n",
      "I0819 05:59:55.004187 140352138418048 efficientnet_model.py:147] round_filter input=40 output=64\n",
      "I0819 05:59:56.181426 140352138418048 efficientnet_model.py:147] round_filter input=40 output=64\n",
      "I0819 05:59:56.181658 140352138418048 efficientnet_model.py:147] round_filter input=80 output=128\n",
      "I0819 05:59:57.827640 140352138418048 efficientnet_model.py:147] round_filter input=80 output=128\n",
      "I0819 05:59:57.827911 140352138418048 efficientnet_model.py:147] round_filter input=112 output=176\n",
      "I0819 05:59:59.455924 140352138418048 efficientnet_model.py:147] round_filter input=112 output=176\n",
      "I0819 05:59:59.456146 140352138418048 efficientnet_model.py:147] round_filter input=192 output=304\n",
      "I0819 06:00:02.009980 140352138418048 efficientnet_model.py:147] round_filter input=192 output=304\n",
      "I0819 06:00:02.010213 140352138418048 efficientnet_model.py:147] round_filter input=320 output=512\n",
      "I0819 06:00:02.692903 140352138418048 efficientnet_model.py:147] round_filter input=1280 output=2048\n",
      "I0819 06:00:02.779003 140352138418048 efficientnet_model.py:458] Building model efficientnet with params ModelConfig(width_coefficient=1.6, depth_coefficient=2.2, resolution=456, dropout_rate=0.4, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
      "I0819 06:00:02.981097 140352138418048 ssd_efficientnet_bifpn_feature_extractor.py:143] EfficientDet EfficientNet backbone version: efficientnet-b6\n",
      "I0819 06:00:02.981348 140352138418048 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet BiFPN num filters: 384\n",
      "I0819 06:00:02.981453 140352138418048 ssd_efficientnet_bifpn_feature_extractor.py:146] EfficientDet BiFPN num iterations: 8\n",
      "I0819 06:00:02.988570 140352138418048 efficientnet_model.py:147] round_filter input=32 output=56\n",
      "I0819 06:00:03.042701 140352138418048 efficientnet_model.py:147] round_filter input=32 output=56\n",
      "I0819 06:00:03.042901 140352138418048 efficientnet_model.py:147] round_filter input=16 output=32\n",
      "I0819 06:00:03.559871 140352138418048 efficientnet_model.py:147] round_filter input=16 output=32\n",
      "I0819 06:00:03.560118 140352138418048 efficientnet_model.py:147] round_filter input=24 output=40\n",
      "I0819 06:00:04.974758 140352138418048 efficientnet_model.py:147] round_filter input=24 output=40\n",
      "I0819 06:00:04.974983 140352138418048 efficientnet_model.py:147] round_filter input=40 output=72\n",
      "I0819 06:00:06.390684 140352138418048 efficientnet_model.py:147] round_filter input=40 output=72\n",
      "I0819 06:00:06.390960 140352138418048 efficientnet_model.py:147] round_filter input=80 output=144\n",
      "I0819 06:00:08.265016 140352138418048 efficientnet_model.py:147] round_filter input=80 output=144\n",
      "I0819 06:00:08.265268 140352138418048 efficientnet_model.py:147] round_filter input=112 output=200\n",
      "I0819 06:00:10.169481 140352138418048 efficientnet_model.py:147] round_filter input=112 output=200\n",
      "I0819 06:00:10.169742 140352138418048 efficientnet_model.py:147] round_filter input=192 output=344\n",
      "I0819 06:00:12.843028 140352138418048 efficientnet_model.py:147] round_filter input=192 output=344\n",
      "I0819 06:00:12.843304 140352138418048 efficientnet_model.py:147] round_filter input=320 output=576\n",
      "I0819 06:00:13.575460 140352138418048 efficientnet_model.py:147] round_filter input=1280 output=2304\n",
      "I0819 06:00:13.673771 140352138418048 efficientnet_model.py:458] Building model efficientnet with params ModelConfig(width_coefficient=1.8, depth_coefficient=2.6, resolution=528, dropout_rate=0.5, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
      "I0819 06:00:13.888427 140352138418048 ssd_efficientnet_bifpn_feature_extractor.py:143] EfficientDet EfficientNet backbone version: efficientnet-b7\n",
      "I0819 06:00:13.888643 140352138418048 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet BiFPN num filters: 384\n",
      "I0819 06:00:13.888794 140352138418048 ssd_efficientnet_bifpn_feature_extractor.py:146] EfficientDet BiFPN num iterations: 8\n",
      "I0819 06:00:13.895777 140352138418048 efficientnet_model.py:147] round_filter input=32 output=64\n",
      "I0819 06:00:13.952215 140352138418048 efficientnet_model.py:147] round_filter input=32 output=64\n",
      "I0819 06:00:13.952439 140352138418048 efficientnet_model.py:147] round_filter input=16 output=32\n",
      "I0819 06:00:14.695377 140352138418048 efficientnet_model.py:147] round_filter input=16 output=32\n",
      "I0819 06:00:14.695604 140352138418048 efficientnet_model.py:147] round_filter input=24 output=48\n",
      "I0819 06:00:16.910310 140352138418048 efficientnet_model.py:147] round_filter input=24 output=48\n",
      "I0819 06:00:16.910542 140352138418048 efficientnet_model.py:147] round_filter input=40 output=80\n",
      "I0819 06:00:18.603055 140352138418048 efficientnet_model.py:147] round_filter input=40 output=80\n",
      "I0819 06:00:18.603280 140352138418048 efficientnet_model.py:147] round_filter input=80 output=160\n",
      "I0819 06:00:21.046203 140352138418048 efficientnet_model.py:147] round_filter input=80 output=160\n",
      "I0819 06:00:21.046419 140352138418048 efficientnet_model.py:147] round_filter input=112 output=224\n",
      "I0819 06:00:23.569803 140352138418048 efficientnet_model.py:147] round_filter input=112 output=224\n",
      "I0819 06:00:23.570044 140352138418048 efficientnet_model.py:147] round_filter input=192 output=384\n",
      "I0819 06:00:26.819068 140352138418048 efficientnet_model.py:147] round_filter input=192 output=384\n",
      "I0819 06:00:26.819316 140352138418048 efficientnet_model.py:147] round_filter input=320 output=640\n",
      "I0819 06:00:27.819319 140352138418048 efficientnet_model.py:147] round_filter input=1280 output=2560\n",
      "I0819 06:00:27.913969 140352138418048 efficientnet_model.py:458] Building model efficientnet with params ModelConfig(width_coefficient=2.0, depth_coefficient=3.1, resolution=600, dropout_rate=0.5, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
      "[       OK ] ModelBuilderTF2Test.test_create_ssd_models_from_config\n",
      "[ RUN      ] ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update\n",
      "[       OK ] ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update\n",
      "[ RUN      ] ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold\n",
      "[       OK ] ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold\n",
      "[ RUN      ] ModelBuilderTF2Test.test_invalid_model_config_proto\n",
      "[       OK ] ModelBuilderTF2Test.test_invalid_model_config_proto\n",
      "[ RUN      ] ModelBuilderTF2Test.test_invalid_second_stage_batch_size\n",
      "[       OK ] ModelBuilderTF2Test.test_invalid_second_stage_batch_size\n",
      "[ RUN      ] ModelBuilderTF2Test.test_session\n",
      "[  SKIPPED ] ModelBuilderTF2Test.test_session\n",
      "[ RUN      ] ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor\n",
      "[       OK ] ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor\n",
      "[ RUN      ] ModelBuilderTF2Test.test_unknown_meta_architecture\n",
      "[       OK ] ModelBuilderTF2Test.test_unknown_meta_architecture\n",
      "[ RUN      ] ModelBuilderTF2Test.test_unknown_ssd_feature_extractor\n",
      "[       OK ] ModelBuilderTF2Test.test_unknown_ssd_feature_extractor\n",
      "----------------------------------------------------------------------\n",
      "Ran 24 tests in 77.015s\n",
      "\n",
      "OK (skipped=1)\n"
     ]
    }
   ],
   "source": [
    "!python object_detection/builders/model_builder_tf2_test.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WmY1el8TaAjB"
   },
   "source": [
    "## Checklist\n",
    "- [x] Cloning tensorflow/model repository\n",
    "- [x] Installing protbuf & cocoapi\n",
    "- [x] Defining proper folder structure\n",
    "- [x] Installing TFOD API\n",
    "- [] Instaling pre-trained-model\n",
    "- [] Creating xml files of Images via labelimg\n",
    "- [] Converting xml files to tfrecords and creating label map\n",
    "- [] Training the model\n",
    "- [] Exporting the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "orcYRjzoaMgP"
   },
   "source": [
    "# Instaling pre-trained-model\n",
    "\n",
    "---\n",
    "\n",
    "Changing the directory for downloading model in the appropriate directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zeO5eIFcT1ed",
    "outputId": "6e2ae2aa-99b3-4573-c0dc-e9d5bbe16ada"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/training_demo/pre-trained-models\n"
     ]
    }
   ],
   "source": [
    "cd /content/training_demo/pre-trained-models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "50mJyq-efDoS",
    "outputId": "82dde6e3-d83a-489e-900d-b8da7796e42f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device: /device:GPU:0\n",
      "2.2.0\n"
     ]
    }
   ],
   "source": [
    "'''Checking for GPU availability'''\n",
    "import tensorflow as tf\n",
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "    print(tf.__version__)\n",
    "else:\n",
    "    print(\"Please install GPU version of TF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3LlMNRXgUOZW",
    "outputId": "2e539734-84f2-4f3a-a893-9011c93ecfd1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-08-19 06:01:17--  http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_resnet101_v1_fpn_640x640_coco17_tpu-8.tar.gz\n",
      "Resolving download.tensorflow.org (download.tensorflow.org)... 64.233.187.128, 2404:6800:4008:c07::80\n",
      "Connecting to download.tensorflow.org (download.tensorflow.org)|64.233.187.128|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 386527459 (369M) [application/x-tar]\n",
      "Saving to: ‘ssd_resnet101_v1_fpn_640x640_coco17_tpu-8.tar.gz’\n",
      "\n",
      "ssd_resnet101_v1_fp 100%[===================>] 368.62M  56.0MB/s    in 6.6s    \n",
      "\n",
      "2021-08-19 06:01:26 (56.0 MB/s) - ‘ssd_resnet101_v1_fpn_640x640_coco17_tpu-8.tar.gz’ saved [386527459/386527459]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_resnet101_v1_fpn_640x640_coco17_tpu-8.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dPuuEPt3anHV"
   },
   "source": [
    "## Unzipping pre-trained model's file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1nQFJCwSUQgn",
    "outputId": "8336acc7-8002-4bce-b414-1a94df81cec0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ssd_resnet101_v1_fpn_640x640_coco17_tpu-8/\n",
      "ssd_resnet101_v1_fpn_640x640_coco17_tpu-8/checkpoint/\n",
      "ssd_resnet101_v1_fpn_640x640_coco17_tpu-8/checkpoint/ckpt-0.data-00000-of-00001\n",
      "ssd_resnet101_v1_fpn_640x640_coco17_tpu-8/checkpoint/checkpoint\n",
      "ssd_resnet101_v1_fpn_640x640_coco17_tpu-8/checkpoint/ckpt-0.index\n",
      "ssd_resnet101_v1_fpn_640x640_coco17_tpu-8/pipeline.config\n",
      "ssd_resnet101_v1_fpn_640x640_coco17_tpu-8/saved_model/\n",
      "ssd_resnet101_v1_fpn_640x640_coco17_tpu-8/saved_model/saved_model.pb\n",
      "ssd_resnet101_v1_fpn_640x640_coco17_tpu-8/saved_model/assets/\n",
      "ssd_resnet101_v1_fpn_640x640_coco17_tpu-8/saved_model/variables/\n",
      "ssd_resnet101_v1_fpn_640x640_coco17_tpu-8/saved_model/variables/variables.data-00000-of-00001\n",
      "ssd_resnet101_v1_fpn_640x640_coco17_tpu-8/saved_model/variables/variables.index\n"
     ]
    }
   ],
   "source": [
    "!tar -xvf ssd_resnet101_v1_fpn_640x640_coco17_tpu-8.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C-jS1ApyazHE"
   },
   "source": [
    "## Checklist\n",
    "- [x] Cloning tensorflow/model repository\n",
    "- [x] Installing protbuf & cocoapi\n",
    "- [x] Defining proper folder structure\n",
    "- [x] Installing TFOD API\n",
    "- [x] Instaling pre-trained-model\n",
    "- [] Creating xml files of Images via labelimg\n",
    "- [] Converting xml files to tfrecords and creating label map\n",
    "- [] Training the model\n",
    "- [] Exporting the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qEbXGVCsa4RU"
   },
   "source": [
    "# Creating xml files of Images via labelimg\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wk05W1Una8i1"
   },
   "source": [
    "Download labelimg and follow its setup steps in your local machine. Then put the training images with their xml files into `/content/training_demo/images/train` and test images with their xml files into `/content/training_demo/images/test`.\n",
    "\n",
    "labelimg can be downloaded from [HERE](https://github.com/tzutalin/labelImg) by following instructions in the readme."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OI8tBvMKbrW_"
   },
   "source": [
    "## Checklist\n",
    "- [x] Cloning tensorflow/model repository\n",
    "- [x] Installing protbuf & cocoapi\n",
    "- [x] Defining proper folder structure\n",
    "- [x] Installing TFOD API\n",
    "- [x] Instaling pre-trained-model\n",
    "- [x] Creating xml files of Images via labelimg\n",
    "- [] Converting xml files to tfrecords and creating label map\n",
    "- [] Training the model\n",
    "- [] Exporting the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_gTKXBGMbyrW"
   },
   "source": [
    "# Converting xml files to tfrecords and creating the label map\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rbuqvx3ccHGA"
   },
   "source": [
    "In this tutorial, I'll be only detecting the instances of the class `human`, so create the label map accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "dT19QTMqUhZc"
   },
   "outputs": [],
   "source": [
    "labels = [{'name': 'human', 'id': 1}]\n",
    "\n",
    "with open('/content/training_demo/annotations/label_map.pbtxt', 'w') as f:\n",
    "  for label in labels:\n",
    "        f.write('item { \\n')\n",
    "        f.write('\\tname:\\'{}\\'\\n'.format(label['name']))\n",
    "        f.write('\\tid:{}\\n'.format(label['id']))\n",
    "        f.write('}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x80Vhw82UjMv",
    "outputId": "65d5731d-b203-4124-f8a3-0fc96cb746a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n"
     ]
    }
   ],
   "source": [
    "cd /content/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yq6khMOTcjEd"
   },
   "source": [
    "We will clone another git repository for setup files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1sYqL0y8Uoff",
    "outputId": "da660105-ee84-42e6-e6ba-3f9f3aed7aa4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'TFOD-API-setup'...\n",
      "remote: Enumerating objects: 6, done.\u001b[K\n",
      "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
      "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
      "remote: Total 6 (delta 0), reused 0 (delta 0), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (6/6), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/mayureshagashe2105/TFOD-API-setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sRZCZmC5Wqq4"
   },
   "source": [
    "### Move all the files from `/content/TFOD-API-setup` to `/content/training_demo`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9qFgK4IUWv-P",
    "outputId": "5d89172e-d481-4036-cf76-a696d95730c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/training_demo\n"
     ]
    }
   ],
   "source": [
    "cd /content/training_demo/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KxZCdz5JUkkb",
    "outputId": "f6ecbe43-547e-4969-8905-c89bef3771c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created the TFRecord file: /content/training_demo/annotations/train.record\n",
      "Successfully created the TFRecord file: /content/training_demo/annotations/test.record\n"
     ]
    }
   ],
   "source": [
    "# Create train data:\n",
    "!python generate_tfrecord.py -x /content/training_demo/images/train -l /content/training_demo/annotations/label_map.pbtxt -o /content/training_demo/annotations/train.record\n",
    "\n",
    "# Create test data:\n",
    "!python generate_tfrecord.py -x /content/training_demo/images/test -l /content/training_demo/annotations/label_map.pbtxt -o /content/training_demo/annotations/test.record\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XQ3IeHBOczfl"
   },
   "source": [
    "## Checklist\n",
    "- [x] Cloning tensorflow/model repository\n",
    "- [x] Installing protbuf & cocoapi\n",
    "- [x] Defining proper folder structure\n",
    "- [x] Installing TFOD API\n",
    "- [x] Instaling pre-trained-model\n",
    "- [x] Creating xml files of Images via labelimg\n",
    "- [x] Converting xml files to tfrecords and creating label map\n",
    "- [] Training the model\n",
    "- [] Exporting the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uJJ7CXSRc5Bd"
   },
   "source": [
    "#  Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5wLNY-3sXANR"
   },
   "source": [
    "Create a folder in /content/training_demo/models/ named `my_ssd_resnet101_v1_fpn`\n",
    " as we will be using \"ssd_resnet101_v1_fpn\" from tf model zoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "JRIqTXmGW_zA"
   },
   "outputs": [],
   "source": [
    "SUB_DIR = os.path.join('/content/training_demo', 'models', 'my_ssd_resnet101_v1_fpn')\n",
    "!mkdir {SUB_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qt5FXkuqYCCw"
   },
   "source": [
    "## All about Config File\n",
    "\n",
    "Pipeline.config is crucial in the training process as it serves as a hyper-parameter tuner and initializer.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "- From `/content/training_demo/pre-trained-models/ssd_resnet101_v1_fpn_640x640_coco17_tpu-8` download `pipline.config`\n",
    "- Upload this downloaded file to `/content/training_demo/models/my_ssd_resnet101_v1_fpn`\n",
    "\n",
    "---\n",
    "\n",
    "### Now make the following changes in the pipline.config present at:\n",
    "\n",
    "`/content/training_demo/models/my_ssd_resnet101_v1_fpn/` \n",
    "\n",
    "  \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Line Number| Field Name | Current Value | Value to insert |\n",
    " :-:|:-:|:-:|:-:\n",
    " 3 | num_classes | 90 | 1 |\n",
    " 6 | height | 640 | 512 |\n",
    " 7 | width | 640 | 512 |\n",
    " 131 | batch_size | 64 | 4 |\n",
    " 152 | total_steps | 25000 | 2000 |\n",
    " 161 | fine_tune_checkpoint | PATH_TO_BE_CONFIGURED | /content/training_demo/pre-trained-models/ssd_resnet101_v1_fpn_640x640_coco17_tpu-8/checkpoint/ckpt-0 |\n",
    " 162 | num_steps | 25000 | 2000 |\n",
    " 167 | fine_tune_checkpoint_type | classification | detection |\n",
    " 168 | use_bfloats16 | true | false\n",
    " 172 | label_map_path | PATH_TO_BE_CONFIGURED | /content/training_demo/annotations/label_map.pbtxt |\n",
    "174 | input_path | PATH_TO_BE_CONFIGURED | /content/training_demo/annotations/train.record |\n",
    "182 | label_map_path | PATH_TO_BE_CONFIGURED | /content/training_demo/annotations/label_map.pbtxt |\n",
    "186 | input_path | PATH_TO_BE_CONFIGURED | /content/training_demo/annotations/test.record |\n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "yBr-VWfMUnBY",
    "outputId": "94279cbb-aabf-4c6f-a173-a9e658ed7860"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/content/training_demo'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "haBwzxdQW4a7",
    "outputId": "df35f0e5-0d4d-4583-e4aa-a0c0c7d2eb14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mannotations\u001b[0m/         export_tflite_graph_tf2.py  model_main_tf2.py\n",
      "\u001b[01;34mexported_models\u001b[0m/     generate_tfrecord.py        \u001b[01;34mmodels\u001b[0m/\n",
      "exporter_main_v2.py  \u001b[01;34mimages\u001b[0m/                     \u001b[01;34mpre-trained-models\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mzGsxt3IhnUn"
   },
   "source": [
    "### Training Script\n",
    "\n",
    "---\n",
    "\n",
    "caution:-  \n",
    "    This process may take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jWWfLEV1W6IJ",
    "outputId": "1ebeb15f-852a-4705-fb31-6cff22e9176e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-19 06:06:20.133932: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
      "2021-08-19 06:06:20.140559: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-08-19 06:06:20.141370: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n",
      "pciBusID: 0000:00:04.0 name: Tesla K80 computeCapability: 3.7\n",
      "coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s\n",
      "2021-08-19 06:06:20.141695: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2021-08-19 06:06:20.143841: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "2021-08-19 06:06:20.145887: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
      "2021-08-19 06:06:20.146284: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
      "2021-08-19 06:06:20.148578: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
      "2021-08-19 06:06:20.149905: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
      "2021-08-19 06:06:20.154383: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2021-08-19 06:06:20.154542: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-08-19 06:06:20.155367: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-08-19 06:06:20.156103: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n",
      "2021-08-19 06:06:20.156579: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2021-08-19 06:06:20.162155: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2299995000 Hz\n",
      "2021-08-19 06:06:20.162418: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x558cb2a14840 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-08-19 06:06:20.162457: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-08-19 06:06:20.266517: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-08-19 06:06:20.267436: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x558cb2a14680 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2021-08-19 06:06:20.267472: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
      "2021-08-19 06:06:20.267716: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-08-19 06:06:20.268444: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n",
      "pciBusID: 0000:00:04.0 name: Tesla K80 computeCapability: 3.7\n",
      "coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s\n",
      "2021-08-19 06:06:20.268533: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2021-08-19 06:06:20.268578: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "2021-08-19 06:06:20.268631: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
      "2021-08-19 06:06:20.268689: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
      "2021-08-19 06:06:20.268743: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
      "2021-08-19 06:06:20.268797: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
      "2021-08-19 06:06:20.268866: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2021-08-19 06:06:20.268967: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-08-19 06:06:20.269773: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-08-19 06:06:20.270491: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n",
      "2021-08-19 06:06:20.270562: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2021-08-19 06:06:20.272310: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-08-19 06:06:20.272345: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \n",
      "2021-08-19 06:06:20.272363: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N \n",
      "2021-08-19 06:06:20.272517: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-08-19 06:06:20.273303: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-08-19 06:06:20.274014: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2021-08-19 06:06:20.274070: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10643 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "I0819 06:06:20.276169 139855536928640 mirrored_strategy.py:500] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "INFO:tensorflow:Maybe overwriting train_steps: None\n",
      "I0819 06:06:20.281302 139855536928640 config_util.py:552] Maybe overwriting train_steps: None\n",
      "INFO:tensorflow:Maybe overwriting use_bfloat16: False\n",
      "I0819 06:06:20.281469 139855536928640 config_util.py:552] Maybe overwriting use_bfloat16: False\n",
      "INFO:tensorflow:Reading unweighted datasets: ['/content/training_demo/annotations/train.record']\n",
      "I0819 06:06:20.694510 139855536928640 dataset_builder.py:163] Reading unweighted datasets: ['/content/training_demo/annotations/train.record']\n",
      "INFO:tensorflow:Reading record datasets for input file: ['/content/training_demo/annotations/train.record']\n",
      "I0819 06:06:20.695202 139855536928640 dataset_builder.py:80] Reading record datasets for input file: ['/content/training_demo/annotations/train.record']\n",
      "INFO:tensorflow:Number of filenames to read: 1\n",
      "I0819 06:06:20.695357 139855536928640 dataset_builder.py:81] Number of filenames to read: 1\n",
      "WARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.\n",
      "W0819 06:06:20.695500 139855536928640 dataset_builder.py:88] num_readers has been reduced to 1 to match input file shards.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/object_detection/builders/dataset_builder.py:105: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`.\n",
      "W0819 06:06:20.697612 139855536928640 deprecation.py:323] From /usr/local/lib/python3.7/dist-packages/object_detection/builders/dataset_builder.py:105: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/object_detection/builders/dataset_builder.py:237: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.map()\n",
      "W0819 06:06:20.715258 139855536928640 deprecation.py:323] From /usr/local/lib/python3.7/dist-packages/object_detection/builders/dataset_builder.py:237: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.map()\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/object_detection/inputs.py:96: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
      "W0819 06:06:35.283896 139855536928640 deprecation.py:323] From /usr/local/lib/python3.7/dist-packages/object_detection/inputs.py:96: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/object_detection/core/preprocessor.py:200: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\n",
      "W0819 06:06:42.484540 139855536928640 deprecation.py:323] From /usr/local/lib/python3.7/dist-packages/object_detection/core/preprocessor.py:200: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/object_detection/inputs.py:282: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "W0819 06:06:45.687346 139855536928640 deprecation.py:323] From /usr/local/lib/python3.7/dist-packages/object_detection/inputs.py:282: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "2021-08-19 06:07:27.404561: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "2021-08-19 06:07:28.753610: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "I0819 06:07:40.238843 139855536928640 cross_device_ops.py:440] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "I0819 06:07:40.240236 139855536928640 cross_device_ops.py:440] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "I0819 06:07:40.241575 139855536928640 cross_device_ops.py:440] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "I0819 06:07:40.242581 139855536928640 cross_device_ops.py:440] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "I0819 06:07:40.243949 139855536928640 cross_device_ops.py:440] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "I0819 06:07:40.244899 139855536928640 cross_device_ops.py:440] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "I0819 06:07:40.246381 139855536928640 cross_device_ops.py:440] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "I0819 06:07:40.247280 139855536928640 cross_device_ops.py:440] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "I0819 06:07:40.248457 139855536928640 cross_device_ops.py:440] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "I0819 06:07:40.249320 139855536928640 cross_device_ops.py:440] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reading unweighted datasets: ['/content/training_demo/annotations/train.record']\n",
      "I0819 06:07:40.459643 139855536928640 dataset_builder.py:163] Reading unweighted datasets: ['/content/training_demo/annotations/train.record']\n",
      "INFO:tensorflow:Reading record datasets for input file: ['/content/training_demo/annotations/train.record']\n",
      "I0819 06:07:40.460374 139855536928640 dataset_builder.py:80] Reading record datasets for input file: ['/content/training_demo/annotations/train.record']\n",
      "INFO:tensorflow:Number of filenames to read: 1\n",
      "I0819 06:07:40.460516 139855536928640 dataset_builder.py:81] Number of filenames to read: 1\n",
      "WARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.\n",
      "W0819 06:07:40.460650 139855536928640 dataset_builder.py:88] num_readers has been reduced to 1 to match input file shards.\n",
      "INFO:tensorflow:Step 100 per-step time 1.903s\n",
      "I0819 06:10:53.325780 139855536928640 model_lib_v2.py:700] Step 100 per-step time 1.903s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.73979676,\n",
      " 'Loss/localization_loss': 0.6514762,\n",
      " 'Loss/regularization_loss': 2.360854,\n",
      " 'Loss/total_loss': 3.752127,\n",
      " 'learning_rate': 0.014666351}\n",
      "I0819 06:10:53.326244 139855536928640 model_lib_v2.py:701] {'Loss/classification_loss': 0.73979676,\n",
      " 'Loss/localization_loss': 0.6514762,\n",
      " 'Loss/regularization_loss': 2.360854,\n",
      " 'Loss/total_loss': 3.752127,\n",
      " 'learning_rate': 0.014666351}\n",
      "INFO:tensorflow:Step 200 per-step time 1.138s\n",
      "I0819 06:12:47.133828 139855536928640 model_lib_v2.py:700] Step 200 per-step time 1.138s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.4961157,\n",
      " 'Loss/localization_loss': 0.54920036,\n",
      " 'Loss/regularization_loss': 2.3365903,\n",
      " 'Loss/total_loss': 3.3819065,\n",
      " 'learning_rate': 0.0159997}\n",
      "I0819 06:12:47.134285 139855536928640 model_lib_v2.py:701] {'Loss/classification_loss': 0.4961157,\n",
      " 'Loss/localization_loss': 0.54920036,\n",
      " 'Loss/regularization_loss': 2.3365903,\n",
      " 'Loss/total_loss': 3.3819065,\n",
      " 'learning_rate': 0.0159997}\n",
      "INFO:tensorflow:Step 300 per-step time 1.129s\n",
      "I0819 06:14:40.040791 139855536928640 model_lib_v2.py:700] Step 300 per-step time 1.129s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.7110893,\n",
      " 'Loss/localization_loss': 0.489148,\n",
      " 'Loss/regularization_loss': 2.308383,\n",
      " 'Loss/total_loss': 3.5086203,\n",
      " 'learning_rate': 0.01733305}\n",
      "I0819 06:14:40.041207 139855536928640 model_lib_v2.py:701] {'Loss/classification_loss': 0.7110893,\n",
      " 'Loss/localization_loss': 0.489148,\n",
      " 'Loss/regularization_loss': 2.308383,\n",
      " 'Loss/total_loss': 3.5086203,\n",
      " 'learning_rate': 0.01733305}\n",
      "INFO:tensorflow:Step 400 per-step time 1.137s\n",
      "I0819 06:16:33.738449 139855536928640 model_lib_v2.py:700] Step 400 per-step time 1.137s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.5436398,\n",
      " 'Loss/localization_loss': 0.42947555,\n",
      " 'Loss/regularization_loss': 2.2781076,\n",
      " 'Loss/total_loss': 3.251223,\n",
      " 'learning_rate': 0.0186664}\n",
      "I0819 06:16:33.738912 139855536928640 model_lib_v2.py:701] {'Loss/classification_loss': 0.5436398,\n",
      " 'Loss/localization_loss': 0.42947555,\n",
      " 'Loss/regularization_loss': 2.2781076,\n",
      " 'Loss/total_loss': 3.251223,\n",
      " 'learning_rate': 0.0186664}\n",
      "INFO:tensorflow:Step 500 per-step time 1.139s\n",
      "I0819 06:18:27.637983 139855536928640 model_lib_v2.py:700] Step 500 per-step time 1.139s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.663496,\n",
      " 'Loss/localization_loss': 0.4026502,\n",
      " 'Loss/regularization_loss': 2.245211,\n",
      " 'Loss/total_loss': 3.311357,\n",
      " 'learning_rate': 0.01999975}\n",
      "I0819 06:18:27.638368 139855536928640 model_lib_v2.py:701] {'Loss/classification_loss': 0.663496,\n",
      " 'Loss/localization_loss': 0.4026502,\n",
      " 'Loss/regularization_loss': 2.245211,\n",
      " 'Loss/total_loss': 3.311357,\n",
      " 'learning_rate': 0.01999975}\n",
      "INFO:tensorflow:Step 600 per-step time 1.137s\n",
      "I0819 06:20:21.380102 139855536928640 model_lib_v2.py:700] Step 600 per-step time 1.137s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.44657215,\n",
      " 'Loss/localization_loss': 0.28213805,\n",
      " 'Loss/regularization_loss': 2.210092,\n",
      " 'Loss/total_loss': 2.9388022,\n",
      " 'learning_rate': 0.0213331}\n",
      "I0819 06:20:21.380513 139855536928640 model_lib_v2.py:701] {'Loss/classification_loss': 0.44657215,\n",
      " 'Loss/localization_loss': 0.28213805,\n",
      " 'Loss/regularization_loss': 2.210092,\n",
      " 'Loss/total_loss': 2.9388022,\n",
      " 'learning_rate': 0.0213331}\n",
      "INFO:tensorflow:Step 700 per-step time 1.140s\n",
      "I0819 06:22:15.406472 139855536928640 model_lib_v2.py:700] Step 700 per-step time 1.140s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.46748593,\n",
      " 'Loss/localization_loss': 0.30383375,\n",
      " 'Loss/regularization_loss': 2.1762486,\n",
      " 'Loss/total_loss': 2.9475682,\n",
      " 'learning_rate': 0.02266645}\n",
      "I0819 06:22:15.406895 139855536928640 model_lib_v2.py:701] {'Loss/classification_loss': 0.46748593,\n",
      " 'Loss/localization_loss': 0.30383375,\n",
      " 'Loss/regularization_loss': 2.1762486,\n",
      " 'Loss/total_loss': 2.9475682,\n",
      " 'learning_rate': 0.02266645}\n",
      "INFO:tensorflow:Step 800 per-step time 1.139s\n",
      "I0819 06:24:09.353389 139855536928640 model_lib_v2.py:700] Step 800 per-step time 1.139s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 1.0249178,\n",
      " 'Loss/localization_loss': 0.35999906,\n",
      " 'Loss/regularization_loss': 2.205589,\n",
      " 'Loss/total_loss': 3.590506,\n",
      " 'learning_rate': 0.023999799}\n",
      "I0819 06:24:09.353808 139855536928640 model_lib_v2.py:701] {'Loss/classification_loss': 1.0249178,\n",
      " 'Loss/localization_loss': 0.35999906,\n",
      " 'Loss/regularization_loss': 2.205589,\n",
      " 'Loss/total_loss': 3.590506,\n",
      " 'learning_rate': 0.023999799}\n",
      "INFO:tensorflow:Step 900 per-step time 1.140s\n",
      "I0819 06:26:03.379537 139855536928640 model_lib_v2.py:700] Step 900 per-step time 1.140s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.49489108,\n",
      " 'Loss/localization_loss': 0.2403951,\n",
      " 'Loss/regularization_loss': 2.1977713,\n",
      " 'Loss/total_loss': 2.9330575,\n",
      " 'learning_rate': 0.025333151}\n",
      "I0819 06:26:03.379967 139855536928640 model_lib_v2.py:701] {'Loss/classification_loss': 0.49489108,\n",
      " 'Loss/localization_loss': 0.2403951,\n",
      " 'Loss/regularization_loss': 2.1977713,\n",
      " 'Loss/total_loss': 2.9330575,\n",
      " 'learning_rate': 0.025333151}\n",
      "INFO:tensorflow:Step 1000 per-step time 1.137s\n",
      "I0819 06:27:57.114207 139855536928640 model_lib_v2.py:700] Step 1000 per-step time 1.137s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.5623992,\n",
      " 'Loss/localization_loss': 0.26068848,\n",
      " 'Loss/regularization_loss': 2.154908,\n",
      " 'Loss/total_loss': 2.9779956,\n",
      " 'learning_rate': 0.0266665}\n",
      "I0819 06:27:57.114615 139855536928640 model_lib_v2.py:701] {'Loss/classification_loss': 0.5623992,\n",
      " 'Loss/localization_loss': 0.26068848,\n",
      " 'Loss/regularization_loss': 2.154908,\n",
      " 'Loss/total_loss': 2.9779956,\n",
      " 'learning_rate': 0.0266665}\n",
      "INFO:tensorflow:Step 1100 per-step time 1.158s\n",
      "I0819 06:29:52.925220 139855536928640 model_lib_v2.py:700] Step 1100 per-step time 1.158s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.40224814,\n",
      " 'Loss/localization_loss': 0.25142714,\n",
      " 'Loss/regularization_loss': 2.1103504,\n",
      " 'Loss/total_loss': 2.7640257,\n",
      " 'learning_rate': 0.02799985}\n",
      "I0819 06:29:52.925629 139855536928640 model_lib_v2.py:701] {'Loss/classification_loss': 0.40224814,\n",
      " 'Loss/localization_loss': 0.25142714,\n",
      " 'Loss/regularization_loss': 2.1103504,\n",
      " 'Loss/total_loss': 2.7640257,\n",
      " 'learning_rate': 0.02799985}\n",
      "INFO:tensorflow:Step 1200 per-step time 1.139s\n",
      "I0819 06:31:46.867021 139855536928640 model_lib_v2.py:700] Step 1200 per-step time 1.139s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.47454438,\n",
      " 'Loss/localization_loss': 0.2236276,\n",
      " 'Loss/regularization_loss': 2.108366,\n",
      " 'Loss/total_loss': 2.806538,\n",
      " 'learning_rate': 0.0293332}\n",
      "I0819 06:31:46.867440 139855536928640 model_lib_v2.py:701] {'Loss/classification_loss': 0.47454438,\n",
      " 'Loss/localization_loss': 0.2236276,\n",
      " 'Loss/regularization_loss': 2.108366,\n",
      " 'Loss/total_loss': 2.806538,\n",
      " 'learning_rate': 0.0293332}\n",
      "INFO:tensorflow:Step 1300 per-step time 1.142s\n",
      "I0819 06:33:41.103186 139855536928640 model_lib_v2.py:700] Step 1300 per-step time 1.142s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.411777,\n",
      " 'Loss/localization_loss': 0.19086397,\n",
      " 'Loss/regularization_loss': 2.06134,\n",
      " 'Loss/total_loss': 2.663981,\n",
      " 'learning_rate': 0.03066655}\n",
      "I0819 06:33:41.103625 139855536928640 model_lib_v2.py:701] {'Loss/classification_loss': 0.411777,\n",
      " 'Loss/localization_loss': 0.19086397,\n",
      " 'Loss/regularization_loss': 2.06134,\n",
      " 'Loss/total_loss': 2.663981,\n",
      " 'learning_rate': 0.03066655}\n",
      "INFO:tensorflow:Step 1400 per-step time 1.141s\n",
      "I0819 06:35:35.161233 139855536928640 model_lib_v2.py:700] Step 1400 per-step time 1.141s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.32992893,\n",
      " 'Loss/localization_loss': 0.19822194,\n",
      " 'Loss/regularization_loss': 2.0129895,\n",
      " 'Loss/total_loss': 2.5411403,\n",
      " 'learning_rate': 0.0319999}\n",
      "I0819 06:35:35.161649 139855536928640 model_lib_v2.py:701] {'Loss/classification_loss': 0.32992893,\n",
      " 'Loss/localization_loss': 0.19822194,\n",
      " 'Loss/regularization_loss': 2.0129895,\n",
      " 'Loss/total_loss': 2.5411403,\n",
      " 'learning_rate': 0.0319999}\n",
      "INFO:tensorflow:Step 1500 per-step time 1.140s\n",
      "I0819 06:37:29.117971 139855536928640 model_lib_v2.py:700] Step 1500 per-step time 1.140s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.46429428,\n",
      " 'Loss/localization_loss': 0.303129,\n",
      " 'Loss/regularization_loss': 1.9646697,\n",
      " 'Loss/total_loss': 2.7320929,\n",
      " 'learning_rate': 0.03333325}\n",
      "I0819 06:37:29.118407 139855536928640 model_lib_v2.py:701] {'Loss/classification_loss': 0.46429428,\n",
      " 'Loss/localization_loss': 0.303129,\n",
      " 'Loss/regularization_loss': 1.9646697,\n",
      " 'Loss/total_loss': 2.7320929,\n",
      " 'learning_rate': 0.03333325}\n",
      "INFO:tensorflow:Step 1600 per-step time 1.139s\n",
      "I0819 06:39:22.976121 139855536928640 model_lib_v2.py:700] Step 1600 per-step time 1.139s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.4350353,\n",
      " 'Loss/localization_loss': 0.21704622,\n",
      " 'Loss/regularization_loss': 1.9153851,\n",
      " 'Loss/total_loss': 2.5674667,\n",
      " 'learning_rate': 0.034666598}\n",
      "I0819 06:39:22.976528 139855536928640 model_lib_v2.py:701] {'Loss/classification_loss': 0.4350353,\n",
      " 'Loss/localization_loss': 0.21704622,\n",
      " 'Loss/regularization_loss': 1.9153851,\n",
      " 'Loss/total_loss': 2.5674667,\n",
      " 'learning_rate': 0.034666598}\n",
      "INFO:tensorflow:Step 1700 per-step time 1.141s\n",
      "I0819 06:41:17.043073 139855536928640 model_lib_v2.py:700] Step 1700 per-step time 1.141s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.4335987,\n",
      " 'Loss/localization_loss': 0.15067372,\n",
      " 'Loss/regularization_loss': 1.8646263,\n",
      " 'Loss/total_loss': 2.4488988,\n",
      " 'learning_rate': 0.03599995}\n",
      "I0819 06:41:17.043437 139855536928640 model_lib_v2.py:701] {'Loss/classification_loss': 0.4335987,\n",
      " 'Loss/localization_loss': 0.15067372,\n",
      " 'Loss/regularization_loss': 1.8646263,\n",
      " 'Loss/total_loss': 2.4488988,\n",
      " 'learning_rate': 0.03599995}\n",
      "INFO:tensorflow:Step 1800 per-step time 1.143s\n",
      "I0819 06:43:11.361123 139855536928640 model_lib_v2.py:700] Step 1800 per-step time 1.143s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.27538452,\n",
      " 'Loss/localization_loss': 0.13291238,\n",
      " 'Loss/regularization_loss': 1.8138429,\n",
      " 'Loss/total_loss': 2.2221398,\n",
      " 'learning_rate': 0.037333302}\n",
      "I0819 06:43:11.361516 139855536928640 model_lib_v2.py:701] {'Loss/classification_loss': 0.27538452,\n",
      " 'Loss/localization_loss': 0.13291238,\n",
      " 'Loss/regularization_loss': 1.8138429,\n",
      " 'Loss/total_loss': 2.2221398,\n",
      " 'learning_rate': 0.037333302}\n",
      "INFO:tensorflow:Step 1900 per-step time 1.139s\n",
      "I0819 06:45:05.253556 139855536928640 model_lib_v2.py:700] Step 1900 per-step time 1.139s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.37660563,\n",
      " 'Loss/localization_loss': 0.18649937,\n",
      " 'Loss/regularization_loss': 1.7622939,\n",
      " 'Loss/total_loss': 2.325399,\n",
      " 'learning_rate': 0.03866665}\n",
      "I0819 06:45:05.253946 139855536928640 model_lib_v2.py:701] {'Loss/classification_loss': 0.37660563,\n",
      " 'Loss/localization_loss': 0.18649937,\n",
      " 'Loss/regularization_loss': 1.7622939,\n",
      " 'Loss/total_loss': 2.325399,\n",
      " 'learning_rate': 0.03866665}\n",
      "INFO:tensorflow:Step 2000 per-step time 1.142s\n",
      "I0819 06:46:59.464837 139855536928640 model_lib_v2.py:700] Step 2000 per-step time 1.142s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.35621253,\n",
      " 'Loss/localization_loss': 0.069648124,\n",
      " 'Loss/regularization_loss': 1.7106653,\n",
      " 'Loss/total_loss': 2.136526,\n",
      " 'learning_rate': nan}\n",
      "I0819 06:46:59.465297 139855536928640 model_lib_v2.py:701] {'Loss/classification_loss': 0.35621253,\n",
      " 'Loss/localization_loss': 0.069648124,\n",
      " 'Loss/regularization_loss': 1.7106653,\n",
      " 'Loss/total_loss': 2.136526,\n",
      " 'learning_rate': nan}\n"
     ]
    }
   ],
   "source": [
    "!python model_main_tf2.py --model_dir=/content/training_demo/models/my_ssd_resnet101_v1_fpn --pipeline_config_path=/content/training_demo/models/my_ssd_resnet101_v1_fpn/pipeline.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "fCMdJUl4W7-k",
    "outputId": "d857dd59-455b-45a6-9770-c4738458405c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/content/training_demo'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LFazpbx7h1YF"
   },
   "source": [
    "## Checklist\n",
    "- [x] Cloning tensorflow/model repository\n",
    "- [x] Installing protbuf & cocoapi\n",
    "- [x] Defining proper folder structure\n",
    "- [x] Installing TFOD API\n",
    "- [x] Instaling pre-trained-model\n",
    "- [x] Creating xml files of Images via labelimg\n",
    "- [x] Converting xml files to tfrecords and creating label map\n",
    "- [x] Training the model\n",
    "- [] Exporting the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mm2iNZWuh7Ft"
   },
   "source": [
    "# Exporting the model\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Upgrade tensrflow to `2.4.1` to ensure no dependency issues while exporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "k0t1iHkHq3-C",
    "outputId": "f2e9f6f6-6848-406b-d026-3302f0788f42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==2.4.1\n",
      "  Downloading tensorflow-2.4.1-cp37-cp37m-manylinux2010_x86_64.whl (394.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 394.3 MB 14 kB/s \n",
      "\u001b[?25hRequirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (3.7.4.3)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (1.6.3)\n",
      "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (1.19.5)\n",
      "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (0.3.3)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (1.12.1)\n",
      "Collecting tensorflow-estimator<2.5.0,>=2.4.0\n",
      "  Downloading tensorflow_estimator-2.4.0-py2.py3-none-any.whl (462 kB)\n",
      "\u001b[K     |████████████████████████████████| 462 kB 48.5 MB/s \n",
      "\u001b[?25hCollecting grpcio~=1.32.0\n",
      "  Downloading grpcio-1.32.0-cp37-cp37m-manylinux2014_x86_64.whl (3.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.8 MB 41.4 MB/s \n",
      "\u001b[?25hRequirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (0.2.0)\n",
      "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (1.15.0)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (1.12)\n",
      "Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (2.10.0)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (1.1.2)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (3.17.3)\n",
      "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (0.37.0)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (1.1.0)\n",
      "Collecting tensorboard~=2.4\n",
      "  Downloading tensorboard-2.6.0-py3-none-any.whl (5.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.6 MB 33.5 MB/s \n",
      "\u001b[?25hRequirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (0.12.0)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (3.3.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.1) (57.4.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.1) (1.8.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.1) (2.26.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.1) (0.6.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.1) (1.0.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.1) (0.4.5)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.1) (3.3.4)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.1) (1.34.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.1) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.1) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.1) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow==2.4.1) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow==2.4.1) (4.6.4)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.1) (0.4.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.1) (1.24.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.1) (2.10)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.1) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.1) (2021.5.30)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow==2.4.1) (3.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.4->tensorflow==2.4.1) (3.5.0)\n",
      "Installing collected packages: grpcio, tensorflow-estimator, tensorboard, tensorflow\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.34.1\n",
      "    Uninstalling grpcio-1.34.1:\n",
      "      Successfully uninstalled grpcio-1.34.1\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.2.0\n",
      "    Uninstalling tensorflow-estimator-2.2.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.2.0\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.2.2\n",
      "    Uninstalling tensorboard-2.2.2:\n",
      "      Successfully uninstalled tensorboard-2.2.2\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.2.0\n",
      "    Uninstalling tensorflow-2.2.0:\n",
      "      Successfully uninstalled tensorflow-2.2.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tf-models-official 2.6.0 requires tensorflow>=2.5.0, but you have tensorflow 2.4.1 which is incompatible.\n",
      "tensorflow-text 2.5.0 requires tensorflow<2.6,>=2.5.0, but you have tensorflow 2.4.1 which is incompatible.\u001b[0m\n",
      "Successfully installed grpcio-1.32.0 tensorboard-2.6.0 tensorflow-2.4.1 tensorflow-estimator-2.4.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "tensorboard",
         "tensorflow"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "!pip install tensorflow==2.4.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model export script\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X3cmnrL_qStQ",
    "outputId": "074269a7-4c17-4672-bcbe-e82aec6120e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-19 06:48:27.456052: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-08-19 06:48:30.392625: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-08-19 06:48:30.396409: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2021-08-19 06:48:30.402526: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-08-19 06:48:30.403277: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:00:04.0 name: Tesla K80 computeCapability: 3.7\n",
      "coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s\n",
      "2021-08-19 06:48:30.403316: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-08-19 06:48:30.505778: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2021-08-19 06:48:30.505929: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
      "2021-08-19 06:48:30.509019: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2021-08-19 06:48:30.509811: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2021-08-19 06:48:30.514512: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2021-08-19 06:48:30.531597: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
      "2021-08-19 06:48:30.575452: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
      "2021-08-19 06:48:30.575666: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-08-19 06:48:30.576574: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-08-19 06:48:30.577369: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2021-08-19 06:48:30.577811: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-08-19 06:48:30.577979: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-08-19 06:48:30.578113: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-08-19 06:48:30.578926: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:00:04.0 name: Tesla K80 computeCapability: 3.7\n",
      "coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s\n",
      "2021-08-19 06:48:30.578967: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-08-19 06:48:30.579031: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2021-08-19 06:48:30.579079: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
      "2021-08-19 06:48:30.579125: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2021-08-19 06:48:30.579180: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2021-08-19 06:48:30.579219: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2021-08-19 06:48:30.579262: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
      "2021-08-19 06:48:30.579304: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
      "2021-08-19 06:48:30.579398: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-08-19 06:48:30.580216: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-08-19 06:48:30.580988: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2021-08-19 06:48:30.581087: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-08-19 06:48:31.162589: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-08-19 06:48:31.162655: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
      "2021-08-19 06:48:31.162690: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
      "2021-08-19 06:48:31.163003: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-08-19 06:48:31.163972: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-08-19 06:48:31.165020: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-08-19 06:48:31.165967: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2021-08-19 06:48:31.166031: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10568 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/object_detection/exporter_lib_v2.py:106: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.map_fn(fn, elems, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.map_fn(fn, elems))\n",
      "W0819 06:48:31.352874 139798889260928 deprecation.py:604] From /usr/local/lib/python3.7/dist-packages/object_detection/exporter_lib_v2.py:106: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.map_fn(fn, elems, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.map_fn(fn, elems))\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.meta_architectures.ssd_meta_arch.SSDMetaArch object at 0x7f251009f510>, because it is not built.\n",
      "W0819 06:48:55.731048 139798889260928 save_impl.py:78] Skipping full serialization of Keras layer <object_detection.meta_architectures.ssd_meta_arch.SSDMetaArch object at 0x7f251009f510>, because it is not built.\n",
      "2021-08-19 06:49:23.675606: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "INFO:tensorflow:Unsupported signature for serialization: (([(<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c45df190>, TensorSpec(shape=(None, 64, 64, 512), dtype=tf.float32, name='image_features/0/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c45df710>, TensorSpec(shape=(None, 32, 32, 1024), dtype=tf.float32, name='image_features/1/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c45df950>, TensorSpec(shape=(None, 16, 16, 2048), dtype=tf.float32, name='image_features/2/1'))], False), {}).\n",
      "I0819 06:49:43.661884 139798889260928 def_function.py:1170] Unsupported signature for serialization: (([(<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c45df190>, TensorSpec(shape=(None, 64, 64, 512), dtype=tf.float32, name='image_features/0/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c45df710>, TensorSpec(shape=(None, 32, 32, 1024), dtype=tf.float32, name='image_features/1/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c45df950>, TensorSpec(shape=(None, 16, 16, 2048), dtype=tf.float32, name='image_features/2/1'))], False), {}).\n",
      "INFO:tensorflow:Unsupported signature for serialization: (([(<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c46303d0>, TensorSpec(shape=(None, 64, 64, 512), dtype=tf.float32, name='image_features/0/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c46309d0>, TensorSpec(shape=(None, 32, 32, 1024), dtype=tf.float32, name='image_features/1/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c4630b90>, TensorSpec(shape=(None, 16, 16, 2048), dtype=tf.float32, name='image_features/2/1'))], True), {}).\n",
      "I0819 06:49:43.662325 139798889260928 def_function.py:1170] Unsupported signature for serialization: (([(<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c46303d0>, TensorSpec(shape=(None, 64, 64, 512), dtype=tf.float32, name='image_features/0/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c46309d0>, TensorSpec(shape=(None, 32, 32, 1024), dtype=tf.float32, name='image_features/1/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c4630b90>, TensorSpec(shape=(None, 16, 16, 2048), dtype=tf.float32, name='image_features/2/1'))], True), {}).\n",
      "INFO:tensorflow:Unsupported signature for serialization: (([(<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c4597ed0>, TensorSpec(shape=(None, 64, 64, 512), dtype=tf.float32, name='image_features/0/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c459b490>, TensorSpec(shape=(None, 32, 32, 1024), dtype=tf.float32, name='image_features/1/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c459b650>, TensorSpec(shape=(None, 16, 16, 2048), dtype=tf.float32, name='image_features/2/1'))], True), {}).\n",
      "I0819 06:49:43.662645 139798889260928 def_function.py:1170] Unsupported signature for serialization: (([(<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c4597ed0>, TensorSpec(shape=(None, 64, 64, 512), dtype=tf.float32, name='image_features/0/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c459b490>, TensorSpec(shape=(None, 32, 32, 1024), dtype=tf.float32, name='image_features/1/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c459b650>, TensorSpec(shape=(None, 16, 16, 2048), dtype=tf.float32, name='image_features/2/1'))], True), {}).\n",
      "INFO:tensorflow:Unsupported signature for serialization: (([(<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c45ab550>, TensorSpec(shape=(None, 64, 64, 512), dtype=tf.float32, name='image_features/0/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c45abb50>, TensorSpec(shape=(None, 32, 32, 1024), dtype=tf.float32, name='image_features/1/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c45abd10>, TensorSpec(shape=(None, 16, 16, 2048), dtype=tf.float32, name='image_features/2/1'))], False), {}).\n",
      "I0819 06:49:43.662935 139798889260928 def_function.py:1170] Unsupported signature for serialization: (([(<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c45ab550>, TensorSpec(shape=(None, 64, 64, 512), dtype=tf.float32, name='image_features/0/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c45abb50>, TensorSpec(shape=(None, 32, 32, 1024), dtype=tf.float32, name='image_features/1/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c45abd10>, TensorSpec(shape=(None, 16, 16, 2048), dtype=tf.float32, name='image_features/2/1'))], False), {}).\n",
      "INFO:tensorflow:Unsupported signature for serialization: (([(<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c4597ed0>, TensorSpec(shape=(None, 64, 64, 512), dtype=tf.float32, name='image_features/0/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c459b490>, TensorSpec(shape=(None, 32, 32, 1024), dtype=tf.float32, name='image_features/1/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c459b650>, TensorSpec(shape=(None, 16, 16, 2048), dtype=tf.float32, name='image_features/2/1'))], True), {}).\n",
      "I0819 06:49:58.334218 139798889260928 def_function.py:1170] Unsupported signature for serialization: (([(<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c4597ed0>, TensorSpec(shape=(None, 64, 64, 512), dtype=tf.float32, name='image_features/0/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c459b490>, TensorSpec(shape=(None, 32, 32, 1024), dtype=tf.float32, name='image_features/1/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c459b650>, TensorSpec(shape=(None, 16, 16, 2048), dtype=tf.float32, name='image_features/2/1'))], True), {}).\n",
      "INFO:tensorflow:Unsupported signature for serialization: (([(<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c45ab550>, TensorSpec(shape=(None, 64, 64, 512), dtype=tf.float32, name='image_features/0/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c45abb50>, TensorSpec(shape=(None, 32, 32, 1024), dtype=tf.float32, name='image_features/1/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c45abd10>, TensorSpec(shape=(None, 16, 16, 2048), dtype=tf.float32, name='image_features/2/1'))], False), {}).\n",
      "I0819 06:49:58.334586 139798889260928 def_function.py:1170] Unsupported signature for serialization: (([(<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c45ab550>, TensorSpec(shape=(None, 64, 64, 512), dtype=tf.float32, name='image_features/0/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c45abb50>, TensorSpec(shape=(None, 32, 32, 1024), dtype=tf.float32, name='image_features/1/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c45abd10>, TensorSpec(shape=(None, 16, 16, 2048), dtype=tf.float32, name='image_features/2/1'))], False), {}).\n",
      "INFO:tensorflow:Unsupported signature for serialization: (([(<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c45df190>, TensorSpec(shape=(None, 64, 64, 512), dtype=tf.float32, name='image_features/0/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c45df710>, TensorSpec(shape=(None, 32, 32, 1024), dtype=tf.float32, name='image_features/1/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c45df950>, TensorSpec(shape=(None, 16, 16, 2048), dtype=tf.float32, name='image_features/2/1'))], False), {}).\n",
      "I0819 06:49:58.334920 139798889260928 def_function.py:1170] Unsupported signature for serialization: (([(<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c45df190>, TensorSpec(shape=(None, 64, 64, 512), dtype=tf.float32, name='image_features/0/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c45df710>, TensorSpec(shape=(None, 32, 32, 1024), dtype=tf.float32, name='image_features/1/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c45df950>, TensorSpec(shape=(None, 16, 16, 2048), dtype=tf.float32, name='image_features/2/1'))], False), {}).\n",
      "INFO:tensorflow:Unsupported signature for serialization: (([(<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c46303d0>, TensorSpec(shape=(None, 64, 64, 512), dtype=tf.float32, name='image_features/0/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c46309d0>, TensorSpec(shape=(None, 32, 32, 1024), dtype=tf.float32, name='image_features/1/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c4630b90>, TensorSpec(shape=(None, 16, 16, 2048), dtype=tf.float32, name='image_features/2/1'))], True), {}).\n",
      "I0819 06:49:58.335214 139798889260928 def_function.py:1170] Unsupported signature for serialization: (([(<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c46303d0>, TensorSpec(shape=(None, 64, 64, 512), dtype=tf.float32, name='image_features/0/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c46309d0>, TensorSpec(shape=(None, 32, 32, 1024), dtype=tf.float32, name='image_features/1/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c4630b90>, TensorSpec(shape=(None, 16, 16, 2048), dtype=tf.float32, name='image_features/2/1'))], True), {}).\n",
      "INFO:tensorflow:Unsupported signature for serialization: (([(<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c45df190>, TensorSpec(shape=(None, 64, 64, 512), dtype=tf.float32, name='image_features/0/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c45df710>, TensorSpec(shape=(None, 32, 32, 1024), dtype=tf.float32, name='image_features/1/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c45df950>, TensorSpec(shape=(None, 16, 16, 2048), dtype=tf.float32, name='image_features/2/1'))], False), {}).\n",
      "I0819 06:49:58.335544 139798889260928 def_function.py:1170] Unsupported signature for serialization: (([(<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c45df190>, TensorSpec(shape=(None, 64, 64, 512), dtype=tf.float32, name='image_features/0/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c45df710>, TensorSpec(shape=(None, 32, 32, 1024), dtype=tf.float32, name='image_features/1/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c45df950>, TensorSpec(shape=(None, 16, 16, 2048), dtype=tf.float32, name='image_features/2/1'))], False), {}).\n",
      "INFO:tensorflow:Unsupported signature for serialization: (([(<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c46303d0>, TensorSpec(shape=(None, 64, 64, 512), dtype=tf.float32, name='image_features/0/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c46309d0>, TensorSpec(shape=(None, 32, 32, 1024), dtype=tf.float32, name='image_features/1/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c4630b90>, TensorSpec(shape=(None, 16, 16, 2048), dtype=tf.float32, name='image_features/2/1'))], True), {}).\n",
      "I0819 06:49:58.335830 139798889260928 def_function.py:1170] Unsupported signature for serialization: (([(<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c46303d0>, TensorSpec(shape=(None, 64, 64, 512), dtype=tf.float32, name='image_features/0/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c46309d0>, TensorSpec(shape=(None, 32, 32, 1024), dtype=tf.float32, name='image_features/1/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c4630b90>, TensorSpec(shape=(None, 16, 16, 2048), dtype=tf.float32, name='image_features/2/1'))], True), {}).\n",
      "W0819 06:50:06.033833 139798889260928 save.py:241] Found untraced functions such as WeightSharedConvolutionalBoxPredictor_layer_call_and_return_conditional_losses, WeightSharedConvolutionalBoxPredictor_layer_call_fn, WeightSharedConvolutionalBoxHead_layer_call_and_return_conditional_losses, WeightSharedConvolutionalBoxHead_layer_call_fn, WeightSharedConvolutionalBoxPredictor_layer_call_fn while saving (showing 5 of 315). These functions will not be directly callable after loading.\n",
      "INFO:tensorflow:Unsupported signature for serialization: (([(<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c45df190>, TensorSpec(shape=(None, 64, 64, 512), dtype=tf.float32, name='image_features/0/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c45df710>, TensorSpec(shape=(None, 32, 32, 1024), dtype=tf.float32, name='image_features/1/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c45df950>, TensorSpec(shape=(None, 16, 16, 2048), dtype=tf.float32, name='image_features/2/1'))], False), {}).\n",
      "I0819 06:50:06.541141 139798889260928 def_function.py:1170] Unsupported signature for serialization: (([(<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c45df190>, TensorSpec(shape=(None, 64, 64, 512), dtype=tf.float32, name='image_features/0/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c45df710>, TensorSpec(shape=(None, 32, 32, 1024), dtype=tf.float32, name='image_features/1/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c45df950>, TensorSpec(shape=(None, 16, 16, 2048), dtype=tf.float32, name='image_features/2/1'))], False), {}).\n",
      "INFO:tensorflow:Unsupported signature for serialization: (([(<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c46303d0>, TensorSpec(shape=(None, 64, 64, 512), dtype=tf.float32, name='image_features/0/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c46309d0>, TensorSpec(shape=(None, 32, 32, 1024), dtype=tf.float32, name='image_features/1/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c4630b90>, TensorSpec(shape=(None, 16, 16, 2048), dtype=tf.float32, name='image_features/2/1'))], True), {}).\n",
      "I0819 06:50:06.541578 139798889260928 def_function.py:1170] Unsupported signature for serialization: (([(<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c46303d0>, TensorSpec(shape=(None, 64, 64, 512), dtype=tf.float32, name='image_features/0/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c46309d0>, TensorSpec(shape=(None, 32, 32, 1024), dtype=tf.float32, name='image_features/1/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c4630b90>, TensorSpec(shape=(None, 16, 16, 2048), dtype=tf.float32, name='image_features/2/1'))], True), {}).\n",
      "INFO:tensorflow:Unsupported signature for serialization: (([(<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c4597ed0>, TensorSpec(shape=(None, 64, 64, 512), dtype=tf.float32, name='image_features/0/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c459b490>, TensorSpec(shape=(None, 32, 32, 1024), dtype=tf.float32, name='image_features/1/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c459b650>, TensorSpec(shape=(None, 16, 16, 2048), dtype=tf.float32, name='image_features/2/1'))], True), {}).\n",
      "I0819 06:50:06.541900 139798889260928 def_function.py:1170] Unsupported signature for serialization: (([(<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c4597ed0>, TensorSpec(shape=(None, 64, 64, 512), dtype=tf.float32, name='image_features/0/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c459b490>, TensorSpec(shape=(None, 32, 32, 1024), dtype=tf.float32, name='image_features/1/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c459b650>, TensorSpec(shape=(None, 16, 16, 2048), dtype=tf.float32, name='image_features/2/1'))], True), {}).\n",
      "INFO:tensorflow:Unsupported signature for serialization: (([(<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c45ab550>, TensorSpec(shape=(None, 64, 64, 512), dtype=tf.float32, name='image_features/0/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c45abb50>, TensorSpec(shape=(None, 32, 32, 1024), dtype=tf.float32, name='image_features/1/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c45abd10>, TensorSpec(shape=(None, 16, 16, 2048), dtype=tf.float32, name='image_features/2/1'))], False), {}).\n",
      "I0819 06:50:06.542194 139798889260928 def_function.py:1170] Unsupported signature for serialization: (([(<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c45ab550>, TensorSpec(shape=(None, 64, 64, 512), dtype=tf.float32, name='image_features/0/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c45abb50>, TensorSpec(shape=(None, 32, 32, 1024), dtype=tf.float32, name='image_features/1/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c45abd10>, TensorSpec(shape=(None, 16, 16, 2048), dtype=tf.float32, name='image_features/2/1'))], False), {}).\n",
      "INFO:tensorflow:Unsupported signature for serialization: (([(<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c4597ed0>, TensorSpec(shape=(None, 64, 64, 512), dtype=tf.float32, name='image_features/0/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c459b490>, TensorSpec(shape=(None, 32, 32, 1024), dtype=tf.float32, name='image_features/1/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c459b650>, TensorSpec(shape=(None, 16, 16, 2048), dtype=tf.float32, name='image_features/2/1'))], True), {}).\n",
      "I0819 06:50:07.501015 139798889260928 def_function.py:1170] Unsupported signature for serialization: (([(<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c4597ed0>, TensorSpec(shape=(None, 64, 64, 512), dtype=tf.float32, name='image_features/0/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c459b490>, TensorSpec(shape=(None, 32, 32, 1024), dtype=tf.float32, name='image_features/1/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c459b650>, TensorSpec(shape=(None, 16, 16, 2048), dtype=tf.float32, name='image_features/2/1'))], True), {}).\n",
      "INFO:tensorflow:Unsupported signature for serialization: (([(<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c45ab550>, TensorSpec(shape=(None, 64, 64, 512), dtype=tf.float32, name='image_features/0/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c45abb50>, TensorSpec(shape=(None, 32, 32, 1024), dtype=tf.float32, name='image_features/1/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c45abd10>, TensorSpec(shape=(None, 16, 16, 2048), dtype=tf.float32, name='image_features/2/1'))], False), {}).\n",
      "I0819 06:50:07.501400 139798889260928 def_function.py:1170] Unsupported signature for serialization: (([(<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c45ab550>, TensorSpec(shape=(None, 64, 64, 512), dtype=tf.float32, name='image_features/0/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c45abb50>, TensorSpec(shape=(None, 32, 32, 1024), dtype=tf.float32, name='image_features/1/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c45abd10>, TensorSpec(shape=(None, 16, 16, 2048), dtype=tf.float32, name='image_features/2/1'))], False), {}).\n",
      "INFO:tensorflow:Unsupported signature for serialization: (([(<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c45df190>, TensorSpec(shape=(None, 64, 64, 512), dtype=tf.float32, name='image_features/0/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c45df710>, TensorSpec(shape=(None, 32, 32, 1024), dtype=tf.float32, name='image_features/1/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c45df950>, TensorSpec(shape=(None, 16, 16, 2048), dtype=tf.float32, name='image_features/2/1'))], False), {}).\n",
      "I0819 06:50:07.501694 139798889260928 def_function.py:1170] Unsupported signature for serialization: (([(<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c45df190>, TensorSpec(shape=(None, 64, 64, 512), dtype=tf.float32, name='image_features/0/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c45df710>, TensorSpec(shape=(None, 32, 32, 1024), dtype=tf.float32, name='image_features/1/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c45df950>, TensorSpec(shape=(None, 16, 16, 2048), dtype=tf.float32, name='image_features/2/1'))], False), {}).\n",
      "INFO:tensorflow:Unsupported signature for serialization: (([(<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c46303d0>, TensorSpec(shape=(None, 64, 64, 512), dtype=tf.float32, name='image_features/0/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c46309d0>, TensorSpec(shape=(None, 32, 32, 1024), dtype=tf.float32, name='image_features/1/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c4630b90>, TensorSpec(shape=(None, 16, 16, 2048), dtype=tf.float32, name='image_features/2/1'))], True), {}).\n",
      "I0819 06:50:07.501979 139798889260928 def_function.py:1170] Unsupported signature for serialization: (([(<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c46303d0>, TensorSpec(shape=(None, 64, 64, 512), dtype=tf.float32, name='image_features/0/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c46309d0>, TensorSpec(shape=(None, 32, 32, 1024), dtype=tf.float32, name='image_features/1/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c4630b90>, TensorSpec(shape=(None, 16, 16, 2048), dtype=tf.float32, name='image_features/2/1'))], True), {}).\n",
      "INFO:tensorflow:Unsupported signature for serialization: (([(<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c45df190>, TensorSpec(shape=(None, 64, 64, 512), dtype=tf.float32, name='image_features/0/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c45df710>, TensorSpec(shape=(None, 32, 32, 1024), dtype=tf.float32, name='image_features/1/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c45df950>, TensorSpec(shape=(None, 16, 16, 2048), dtype=tf.float32, name='image_features/2/1'))], False), {}).\n",
      "I0819 06:50:07.502289 139798889260928 def_function.py:1170] Unsupported signature for serialization: (([(<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c45df190>, TensorSpec(shape=(None, 64, 64, 512), dtype=tf.float32, name='image_features/0/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c45df710>, TensorSpec(shape=(None, 32, 32, 1024), dtype=tf.float32, name='image_features/1/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c45df950>, TensorSpec(shape=(None, 16, 16, 2048), dtype=tf.float32, name='image_features/2/1'))], False), {}).\n",
      "INFO:tensorflow:Unsupported signature for serialization: (([(<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c46303d0>, TensorSpec(shape=(None, 64, 64, 512), dtype=tf.float32, name='image_features/0/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c46309d0>, TensorSpec(shape=(None, 32, 32, 1024), dtype=tf.float32, name='image_features/1/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c4630b90>, TensorSpec(shape=(None, 16, 16, 2048), dtype=tf.float32, name='image_features/2/1'))], True), {}).\n",
      "I0819 06:50:07.502546 139798889260928 def_function.py:1170] Unsupported signature for serialization: (([(<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c46303d0>, TensorSpec(shape=(None, 64, 64, 512), dtype=tf.float32, name='image_features/0/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c46309d0>, TensorSpec(shape=(None, 32, 32, 1024), dtype=tf.float32, name='image_features/1/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c4630b90>, TensorSpec(shape=(None, 16, 16, 2048), dtype=tf.float32, name='image_features/2/1'))], True), {}).\n",
      "W0819 06:50:09.209246 139798889260928 save.py:241] Found untraced functions such as WeightSharedConvolutionalBoxPredictor_layer_call_and_return_conditional_losses, WeightSharedConvolutionalBoxPredictor_layer_call_fn, WeightSharedConvolutionalBoxHead_layer_call_and_return_conditional_losses, WeightSharedConvolutionalBoxHead_layer_call_fn, WeightSharedConvolutionalBoxPredictor_layer_call_fn while saving (showing 5 of 315). These functions will not be directly callable after loading.\n",
      "INFO:tensorflow:Unsupported signature for serialization: (([(<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c45df190>, TensorSpec(shape=(None, 64, 64, 512), dtype=tf.float32, name='image_features/0/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c45df710>, TensorSpec(shape=(None, 32, 32, 1024), dtype=tf.float32, name='image_features/1/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c45df950>, TensorSpec(shape=(None, 16, 16, 2048), dtype=tf.float32, name='image_features/2/1'))], False), {}).\n",
      "I0819 06:50:18.548742 139798889260928 def_function.py:1170] Unsupported signature for serialization: (([(<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c45df190>, TensorSpec(shape=(None, 64, 64, 512), dtype=tf.float32, name='image_features/0/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c45df710>, TensorSpec(shape=(None, 32, 32, 1024), dtype=tf.float32, name='image_features/1/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c45df950>, TensorSpec(shape=(None, 16, 16, 2048), dtype=tf.float32, name='image_features/2/1'))], False), {}).\n",
      "INFO:tensorflow:Unsupported signature for serialization: (([(<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c46303d0>, TensorSpec(shape=(None, 64, 64, 512), dtype=tf.float32, name='image_features/0/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c46309d0>, TensorSpec(shape=(None, 32, 32, 1024), dtype=tf.float32, name='image_features/1/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c4630b90>, TensorSpec(shape=(None, 16, 16, 2048), dtype=tf.float32, name='image_features/2/1'))], True), {}).\n",
      "I0819 06:50:18.549160 139798889260928 def_function.py:1170] Unsupported signature for serialization: (([(<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c46303d0>, TensorSpec(shape=(None, 64, 64, 512), dtype=tf.float32, name='image_features/0/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c46309d0>, TensorSpec(shape=(None, 32, 32, 1024), dtype=tf.float32, name='image_features/1/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c4630b90>, TensorSpec(shape=(None, 16, 16, 2048), dtype=tf.float32, name='image_features/2/1'))], True), {}).\n",
      "INFO:tensorflow:Unsupported signature for serialization: (([(<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c4597ed0>, TensorSpec(shape=(None, 64, 64, 512), dtype=tf.float32, name='image_features/0/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c459b490>, TensorSpec(shape=(None, 32, 32, 1024), dtype=tf.float32, name='image_features/1/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c459b650>, TensorSpec(shape=(None, 16, 16, 2048), dtype=tf.float32, name='image_features/2/1'))], True), {}).\n",
      "I0819 06:50:18.549842 139798889260928 def_function.py:1170] Unsupported signature for serialization: (([(<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c4597ed0>, TensorSpec(shape=(None, 64, 64, 512), dtype=tf.float32, name='image_features/0/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c459b490>, TensorSpec(shape=(None, 32, 32, 1024), dtype=tf.float32, name='image_features/1/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c459b650>, TensorSpec(shape=(None, 16, 16, 2048), dtype=tf.float32, name='image_features/2/1'))], True), {}).\n",
      "INFO:tensorflow:Unsupported signature for serialization: (([(<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c45ab550>, TensorSpec(shape=(None, 64, 64, 512), dtype=tf.float32, name='image_features/0/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c45abb50>, TensorSpec(shape=(None, 32, 32, 1024), dtype=tf.float32, name='image_features/1/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c45abd10>, TensorSpec(shape=(None, 16, 16, 2048), dtype=tf.float32, name='image_features/2/1'))], False), {}).\n",
      "I0819 06:50:18.550153 139798889260928 def_function.py:1170] Unsupported signature for serialization: (([(<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c45ab550>, TensorSpec(shape=(None, 64, 64, 512), dtype=tf.float32, name='image_features/0/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c45abb50>, TensorSpec(shape=(None, 32, 32, 1024), dtype=tf.float32, name='image_features/1/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f24c45abd10>, TensorSpec(shape=(None, 16, 16, 2048), dtype=tf.float32, name='image_features/2/1'))], False), {}).\n",
      "INFO:tensorflow:Assets written to: /content/training_demo/exported_models/my_model/saved_model/assets\n",
      "I0819 06:50:21.498966 139798889260928 builder_impl.py:775] Assets written to: /content/training_demo/exported_models/my_model/saved_model/assets\n",
      "INFO:tensorflow:Writing pipeline config file to /content/training_demo/exported_models/my_model/pipeline.config\n",
      "I0819 06:50:23.041343 139798889260928 config_util.py:254] Writing pipeline config file to /content/training_demo/exported_models/my_model/pipeline.config\n"
     ]
    }
   ],
   "source": [
    "!python exporter_main_v2.py --input_type image_tensor --pipeline_config_path /content/training_demo/models/my_ssd_resnet101_v1_fpn/pipeline.config --trained_checkpoint_dir /content/training_demo/models/my_ssd_resnet101_v1_fpn --output_directory /content/training_demo/exported_models/my_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X6ZZFymBi7y3"
   },
   "source": [
    "## Checklist\n",
    "- [x] Cloning tensorflow/model repository\n",
    "- [x] Installing protbuf & cocoapi\n",
    "- [x] Defining proper folder structure\n",
    "- [x] Installing TFOD API\n",
    "- [x] Instaling pre-trained-model\n",
    "- [x] Creating xml files of Images via labelimg\n",
    "- [x] Converting xml files to tfrecords and creating label map\n",
    "- [x] Training the model\n",
    "- [x] Exporting the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AOcRNcl-CijR"
   },
   "source": [
    "## Final folder structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jgtwesWECuJB",
    "outputId": "9feee7a8-769c-4c36-ac81-3107b1d9acc2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/training_demo\n",
      "├── annotations\n",
      "│   ├── label_map.pbtxt\n",
      "│   ├── test.record\n",
      "│   └── train.record\n",
      "├── exported_models\n",
      "│   └── my_model\n",
      "│       ├── checkpoint\n",
      "│       │   ├── checkpoint\n",
      "│       │   ├── ckpt-0.data-00000-of-00001\n",
      "│       │   └── ckpt-0.index\n",
      "│       ├── pipeline.config\n",
      "│       └── saved_model\n",
      "│           ├── assets\n",
      "│           ├── saved_model.pb\n",
      "│           └── variables\n",
      "│               ├── variables.data-00000-of-00001\n",
      "│               └── variables.index\n",
      "├── exporter_main_v2.py\n",
      "├── export_tflite_graph_tf2.py\n",
      "├── generate_tfrecord.py\n",
      "├── images\n",
      "│   ├── test\n",
      "│   │   ├── img4.jpg\n",
      "│   │   ├── img4.xml\n",
      "│   │   ├── img6.jpg\n",
      "│   │   ├── img6.xml\n",
      "│   │   ├── img8.jpg\n",
      "│   │   └── img8.xml\n",
      "│   └── train\n",
      "│       ├── img10.jpg\n",
      "│       ├── img10.xml\n",
      "│       ├── img1.jpg\n",
      "│       ├── img1.xml\n",
      "│       ├── img2.jpg\n",
      "│       ├── img2.xml\n",
      "│       ├── img3.jpg\n",
      "│       ├── img3.xml\n",
      "│       ├── img5.jpg\n",
      "│       ├── img5.xml\n",
      "│       ├── img7.jpg\n",
      "│       ├── img7.xml\n",
      "│       ├── img8.jpg\n",
      "│       ├── img8.xml\n",
      "│       ├── img9.jpg\n",
      "│       └── img9.xml\n",
      "├── model_main_tf2.py\n",
      "├── models\n",
      "│   └── my_ssd_resnet101_v1_fpn\n",
      "│       ├── checkpoint\n",
      "│       ├── ckpt-1.data-00000-of-00002\n",
      "│       ├── ckpt-1.data-00001-of-00002\n",
      "│       ├── ckpt-1.index\n",
      "│       ├── ckpt-2.data-00000-of-00002\n",
      "│       ├── ckpt-2.data-00001-of-00002\n",
      "│       ├── ckpt-2.index\n",
      "│       ├── ckpt-3.data-00000-of-00002\n",
      "│       ├── ckpt-3.data-00001-of-00002\n",
      "│       ├── ckpt-3.index\n",
      "│       ├── pipeline.config\n",
      "│       └── train\n",
      "│           └── events.out.tfevents.1629353180.294b9d136c94.789.11.v2\n",
      "└── pre-trained-models\n",
      "    ├── ssd_resnet101_v1_fpn_640x640_coco17_tpu-8\n",
      "    │   ├── checkpoint\n",
      "    │   │   ├── checkpoint\n",
      "    │   │   ├── ckpt-0.data-00000-of-00001\n",
      "    │   │   └── ckpt-0.index\n",
      "    │   ├── pipeline.config\n",
      "    │   └── saved_model\n",
      "    │       ├── assets\n",
      "    │       ├── saved_model.pb\n",
      "    │       └── variables\n",
      "    │           ├── variables.data-00000-of-00001\n",
      "    │           └── variables.index\n",
      "    └── ssd_resnet101_v1_fpn_640x640_coco17_tpu-8.tar.gz\n",
      "\n",
      "19 directories, 56 files\n"
     ]
    }
   ],
   "source": [
    "!tree /content/training_demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yoTeHub7i9ee"
   },
   "source": [
    "Kudos!!! Now you can use your exported_model by downloading the entire `exorted_models` folder."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOVIEmCA1AnLu1DtSeReiyf",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "TFOD API tutorial.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
