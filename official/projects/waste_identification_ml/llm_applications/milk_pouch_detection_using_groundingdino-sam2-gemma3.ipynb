{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAppGMCpM9Qq"
      },
      "source": [
        "# Automatic Mask Generation Using Unsupervised Approach with Grounding Dino, SAM2, and Gemma3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwNkr2MGNGQ3"
      },
      "source": [
        "In this notebook, we build an end-to-end unsupervised pipeline for object detection, segmentation, classification, and tracking—focusing on identifying and following milk pouches without manual labels. This approach leverages cutting-edge vision and language models and concludes with lightweight object tracking based on extracted features from segmentation masks.\n",
        "\n",
        "Key Components:\n",
        "\n",
        "\n",
        "\n",
        "1.   **Grounding Dino**\n",
        "\n",
        "A powerful vision-language model that performs generic object detection by returning bounding boxes around visually significant regions—completely label-free and prompt-driven.\n",
        "\n",
        "2.   **SAM2 (Segment Anything Model v2)**\n",
        "\n",
        "Using the bounding boxes from Grounding Dino, SAM2 generates precise segmentation masks, enabling instance-level understanding and clean extraction of objects.\n",
        "\n",
        "3.  **Gemma3 12B QAT Model**\n",
        "\n",
        "Each cropped masked region is passed to an open source Gemma3 quantization-aware large language model to determine whether it contains a milk pouch or not, enabling robust classification without explicit supervised training.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8JMep1cNw9K"
      },
      "source": [
        "## Install necessary packages.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dHhKN6bfylwZ"
      },
      "outputs": [],
      "source": [
        "!git clone 'https://github.com/IDEA-Research/Grounded-SAM-2'\n",
        "!pip install 'git+https://github.com/IDEA-Research/Grounded-SAM-2'\n",
        "\n",
        "%cd 'Grounded-SAM-2'\n",
        "\n",
        "# Install SAM2\n",
        "!pip install -e .\n",
        "\n",
        "# Install Grounding Dino\n",
        "!pip install --no-build-isolation -e grounding_dino\n",
        "\n",
        "!pip install addict yapf supervision\u003e=0.22.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oiv9cRLHZPMj"
      },
      "outputs": [],
      "source": [
        "# Required for Ollama to detect GPUs.\n",
        "!sudo apt-get install -y pciutils lshw\n",
        "!pip install ollama"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuJRa2S1N2CH"
      },
      "source": [
        "## Import model weights and configuration files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZhkdMt68zPh"
      },
      "outputs": [],
      "source": [
        "# Download Grounding Dino weights.\n",
        "!mkdir grounding_dino_weights\n",
        "!wget -P ./grounding_dino_weights https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth\n",
        "!wget -P ./grounding_dino_weights https://raw.githubusercontent.com/IDEA-Research/GroundingDINO/refs/heads/main/groundingdino/config/GroundingDINO_SwinT_OGC.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvYX8RPyAJ2y"
      },
      "outputs": [],
      "source": [
        "# Download SAM2 weights\n",
        "!mkdir sam2_weights\n",
        "!wget -P ./sam2_weights https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_large.pt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NoNvKYb3AO9-"
      },
      "outputs": [],
      "source": [
        "# download the sample image from the circularnet project\n",
        "url = (\n",
        "    \"https://raw.githubusercontent.com/tensorflow/models/master/official/\"\n",
        "    \"projects/waste_identification_ml/pre_processing/config/sample_images/\"\n",
        "    \"IMG_6509.png\"\n",
        ")\n",
        "\n",
        "!curl -O {url} \u003e /dev/null 2\u003e\u00261"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPO5L6UXN6LO"
      },
      "source": [
        "## Import libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bYtQDcmzAjI-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import supervision as sv\n",
        "import torch\n",
        "import tqdm\n",
        "import numpy as np\n",
        "from torchvision.ops import box_convert\n",
        "from PIL import Image\n",
        "from ollama import chat, ChatResponse\n",
        "import glob\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C0XDBDs0SszU"
      },
      "outputs": [],
      "source": [
        "#@title Utils\n",
        "\n",
        "def show_mask(\n",
        "        mask,\n",
        "        ax,\n",
        "        random_color=False,\n",
        "        borders = True\n",
        "):\n",
        "  if random_color:\n",
        "    color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
        "  else:\n",
        "    color = np.array([30/255, 144/255, 255/255, 0.6])\n",
        "  h, w = mask.shape[-2:]\n",
        "  binary_mask = mask.astype(np.uint8)\n",
        "  mask_image =  binary_mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
        "  if borders:\n",
        "    contours, _ = cv2.findContours(binary_mask,cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
        "    # Try to smooth contours\n",
        "    contours = [cv2.approxPolyDP(contour, epsilon=0.01, closed=True) for contour in contours]\n",
        "    mask_image = cv2.drawContours(mask_image, contours, -1, (1, 1, 1, 0.5), thickness=2)\n",
        "  ax.imshow(mask_image)\n",
        "\n",
        "\n",
        "def show_points(\n",
        "        coords,\n",
        "        labels,\n",
        "        ax,\n",
        "        marker_size=375\n",
        "):\n",
        "  pos_points = coords[labels==1]\n",
        "  neg_points = coords[labels==0]\n",
        "  ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
        "  ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
        "\n",
        "\n",
        "def show_box(box, ax):\n",
        "  x0, y0 = box[0], box[1]\n",
        "  w, h = box[2] - box[0], box[3] - box[1]\n",
        "  ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0, 0, 0, 0), lw=2))\n",
        "\n",
        "\n",
        "def show_masks(\n",
        "        image,\n",
        "        masks,\n",
        "        scores,\n",
        "        point_coords=None,\n",
        "        box_coords=None,\n",
        "        input_labels=None,\n",
        "        borders=True\n",
        "):\n",
        "  for i, (mask, score) in enumerate(zip(masks, scores)):\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.imshow(image)\n",
        "    show_mask(mask, plt.gca(), borders=borders)\n",
        "    if point_coords is not None:\n",
        "      assert input_labels is not None\n",
        "      show_points(point_coords, input_labels, plt.gca())\n",
        "    if box_coords is not None:\n",
        "      # boxes\n",
        "      show_box(box_coords, plt.gca())\n",
        "    if len(scores) \u003e 1:\n",
        "      plt.title(f\"Mask {i+1}, Score: {score:.3f}\", fontsize=18)\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3Opdq9nOYkj"
      },
      "source": [
        "## Load models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JcGgDIar-u_S"
      },
      "outputs": [],
      "source": [
        "# Load Grounding Dino model.\n",
        "from grounding_dino.groundingdino.util.inference import load_model, load_image, predict, annotate\n",
        "\n",
        "# Path to the pre-trained Grounding Dino model checkpoint\n",
        "WEIGHTS_PATH = \"grounding_dino_weights/groundingdino_swint_ogc.pth\"\n",
        "\n",
        "# Path to the configuration file for the Grounding Dino model variant being used\n",
        "CONFIG_PATH = \"grounding_dino_weights/GroundingDINO_SwinT_OGC.py\"\n",
        "\n",
        "model = load_model(CONFIG_PATH, WEIGHTS_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zYIaWLD2OjKD"
      },
      "outputs": [],
      "source": [
        "# Load SAM2 model.\n",
        "from sam2.build_sam import build_sam2\n",
        "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
        "\n",
        "# Path to the pre-trained SAM2 model checkpoint\n",
        "sam2_checkpoint = \"sam2_weights/sam2.1_hiera_large.pt\"\n",
        "\n",
        "# Path to the configuration file for the SAM2 model variant being used\n",
        "model_cfg = \"configs/sam2.1/sam2.1_hiera_l.yaml\"\n",
        "\n",
        "# Build the SAM2 model using the config and checkpoint; `device` should be set to \"cuda\" or \"cpu\"\n",
        "sam2_model = build_sam2(model_cfg, sam2_checkpoint, device=torch.device(\"cuda\"))\n",
        "\n",
        "# Create a predictor object using the loaded SAM2 model for image-based mask prediction\n",
        "sam2_predictor = SAM2ImagePredictor(sam2_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkTUJzhTRCy6"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6b5Z7-XAnkX"
      },
      "outputs": [],
      "source": [
        "# Inference via Grounding Dino\n",
        "%%time\n",
        "IMAGE_PATH = \"IMG_6509.png\"\n",
        "TEXT_PROMPT = \"packet\"\n",
        "BOX_TRESHOLD = 0.25\n",
        "TEXT_TRESHOLD = 0.25\n",
        "\n",
        "image_source, image = load_image(IMAGE_PATH)\n",
        "\n",
        "boxes, logits, phrases = predict(\n",
        "    model=model,\n",
        "    image=image,\n",
        "    caption=TEXT_PROMPT,\n",
        "    box_threshold=BOX_TRESHOLD,\n",
        "    text_threshold=TEXT_TRESHOLD\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fLeaXwJKLaKw"
      },
      "outputs": [],
      "source": [
        "# Visualize Grounding Dino results.\n",
        "annotated_frame = annotate(image_source=image_source, boxes=boxes, logits=logits, phrases=phrases)\n",
        "\n",
        "%matplotlib inline\n",
        "sv.plot_image(annotated_frame, (16, 16))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "shhCTs2tMKqy"
      },
      "outputs": [],
      "source": [
        "# Perform segmentation on bbox cordinates using SAM2 model.\n",
        "sam2_predictor.set_image(image_source)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b5L_XkxCQ5eD"
      },
      "outputs": [],
      "source": [
        "# Create a directory to store the cropped object images.\n",
        "os.makedirs('tempdir', exist_ok=True)\n",
        "\n",
        "# Convert bbox format\n",
        "h, w, _ = image_source.shape\n",
        "boxes = boxes * torch.Tensor([w, h, w, h])\n",
        "xyxy = box_convert(boxes=boxes, in_fmt=\"cxcywh\", out_fmt=\"xyxy\").numpy().astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4GwSB3R8fd5y"
      },
      "outputs": [],
      "source": [
        "for idx, bbox in tqdm.tqdm(enumerate(xyxy)):\n",
        "  x1, y1, x2, y2 = bbox\n",
        "\n",
        "  if (x2-x1)*(y2-y1) \u003c 0.25 * math.prod(image.size):\n",
        "    masks, scores, _ = sam2_predictor.predict(\n",
        "      point_coords=None,\n",
        "      point_labels=None,\n",
        "      box=bbox[None, :],\n",
        "      multimask_output=False,\n",
        "    )\n",
        "\n",
        "    # show_masks(image_source, masks, scores, box_coords=bbox)\n",
        "\n",
        "    # Convert the first mask to 0-255 and expand its dimensions to match the image channels.\n",
        "    # Multiply the mask with the original image (preserves object, sets background to 0).\n",
        "    # Crop the masked image to the bounding box [y1:y2, x1:x2].\n",
        "    masked_object = Image.fromarray(\n",
        "        np.where(\n",
        "            np.expand_dims(masks[0]*255, -1),\n",
        "            image_source, 0\n",
        "        )[y1:y2, x1:x2]\n",
        "    )\n",
        "\n",
        "    image_path = f'tempdir/{os.path.splitext(IMAGE_PATH)[0]}_{idx}.png'\n",
        "    masked_object.save(image_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NHKvcPGXk7n"
      },
      "source": [
        "## Download Gemma3 model using Ollama tool."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCiuCSZoXb-M"
      },
      "source": [
        "Run the following commands in the terminal within your colab notebook.\n",
        "\n",
        "```\n",
        "curl https://ollama.ai/install.sh | sh\n",
        "ollama serve\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MbX_0DHISjAY"
      },
      "outputs": [],
      "source": [
        "# Pull the required open sourced LLM model.\n",
        "!ollama pull gemma3:12b-it-qat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-qOl2eYZbvg"
      },
      "outputs": [],
      "source": [
        "# Check if the model is downloaded.\n",
        "!ollama list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1hKcs1yZdqZ"
      },
      "outputs": [],
      "source": [
        "# Prompt to analyze an image for milk packet vs others.\n",
        "prompt = \"\"\"\n",
        "Analyze the provided image of packaging. Was this packaging used to contain milk or a milk-based product?  Answer in yes or no only.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iF5p-hECZfKA"
      },
      "outputs": [],
      "source": [
        "# Read an cropped images to perform inference using LLM.\n",
        "images = glob.glob('tempdir/*.png')\n",
        "\n",
        "for path in images:\n",
        "  # Run the chat/inference API, sending the temporary masked object image as input.\n",
        "  response: ChatResponse = chat(model='gemma3:12b-it-qat', messages=[\n",
        "    {\n",
        "      'role': 'user',\n",
        "      'content': prompt,\n",
        "      'images': [path]\n",
        "    },\n",
        "  ])\n",
        "  image = cv2.imread(path)\n",
        "  image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "  plt.imshow(image)\n",
        "  plt.axis('off')\n",
        "  plt.show()\n",
        "\n",
        "  # Print the model's response content (the generated answer)\n",
        "  print(f\"\\n{response.message.content}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
