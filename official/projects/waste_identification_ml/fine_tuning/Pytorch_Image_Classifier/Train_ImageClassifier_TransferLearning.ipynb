{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Training a Vision Transformer (ViT) Classifier with Transfer Learning"
      ],
      "metadata": {},
      "id": "5LE_9COIvNxw"
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook demonstrates how to train an image classifier using PyTorch with a V**ision Transformer (ViT) backbone**. We’ll leverage **transfer learning** by starting from pretrained weights and fine-tuning the model for our custom dataset.\\n\n",
        "\n",
        "To make the training process efficient and reliable, we’ll incorporate **callbacks** such as:\n",
        "\n",
        "**Best Model Checkpointing** – automatically save the model state with the lowest validation loss.\n",
        "\n",
        "**Early Stopping** – stop training when validation performance stops improving, preventing overfitting and wasted compute.\n",
        "\n",
        "By the end of this notebook, you’ll have a PyTorch-based image classification pipeline that’s modular, reproducible, and ready for experimentation on custom datasets."
      ],
      "metadata": {},
      "id": "5ZDI75XovKrp"
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect to google drive in case your data is there.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "try:\n",
        "  !ln -s /content/gdrive/My\\ Drive/ /mydrive\n",
        "  print('Successful')\n",
        "except Exception as e:\n",
        "  print(e)\n",
        "  print('Not successful')"
      ],
      "metadata": {},
      "id": "Jh47hJk4YcI8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!pip install torchinfo"
      ],
      "id": "911b8aac",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/AarohiSingla/Image-Classification-Using-Vision-transformer.git\n",
        "%cd Image-Classification-Using-Vision-transformer"
      ],
      "metadata": {},
      "id": "gQyII9g5bPzS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the functions for training with callbacks.\n",
        "url = (\n",
        "    \"https://raw.githubusercontent.com/tensorflow/models/refs/heads/master/\"\n",
        "    \"official/projects/waste_identification_ml/fine_tuning/\"\n",
        "    \"Pytorch_Image_Classifier/training_with_callbacks.py\"\n",
        ")\n",
        "!wget {url} \u003e /dev/null 2\u003e\u00261"
      ],
      "metadata": {},
      "id": "I4qgiwnQxzbS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fb1c10e",
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torchvision\n",
        "from torch import nn\n",
        "from torchvision import transforms\n",
        "import glob\n",
        "import requests\n",
        "import os\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from torchinfo import summary\n",
        "\n",
        "from helper_functions import set_seeds\n",
        "from helper_functions import plot_loss_curves\n",
        "from going_modular.going_modular.predictions import pred_and_plot_image\n",
        "\n",
        "from training_with_callbacks import EarlyStopping\n",
        "import training_with_callbacks\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "num_workers = os.cpu_count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d49225b",
      "metadata": {
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Utils\n",
        "\n",
        "## And now we've got transforms ready, we can turn our images into DataLoaders using the create_dataloaders()\n",
        "def create_dataloaders(\n",
        "    train_dir: str,\n",
        "    test_dir: str,\n",
        "    transform: transforms.Compose,\n",
        "    batch_size: int,\n",
        "    num_workers: int=NUM_WORKERS\n",
        "):\n",
        "\n",
        "  # Use ImageFolder to create dataset(s)\n",
        "  train_data = datasets.ImageFolder(train_dir, transform=transform)\n",
        "  test_data = datasets.ImageFolder(test_dir, transform=transform)\n",
        "\n",
        "  # Get class names\n",
        "  class_names = train_data.classes\n",
        "\n",
        "  # Turn images into data loaders\n",
        "  train_dataloader = DataLoader(\n",
        "      train_data,\n",
        "      batch_size=batch_size,\n",
        "      shuffle=True,\n",
        "      num_workers=num_workers,\n",
        "      pin_memory=True,\n",
        "  )\n",
        "  test_dataloader = DataLoader(\n",
        "      test_data,\n",
        "      batch_size=batch_size,\n",
        "      shuffle=False,\n",
        "      num_workers=num_workers,\n",
        "      pin_memory=True,\n",
        "  )\n",
        "\n",
        "  return train_dataloader, test_dataloader, class_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0de25b1a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get pretrained weights for ViT-Base.\n",
        "pretrained_vit_weights = torchvision.models.ViT_B_16_Weights.DEFAULT\n",
        "\n",
        "# Setup a ViT model instance with pretrained weights.\n",
        "pretrained_vit = torchvision.models.vit_b_16(weights=pretrained_vit_weights).to(device)\n",
        "\n",
        "# Freeze the base parameters.\n",
        "for parameter in pretrained_vit.parameters():\n",
        "    parameter.requires_grad = False\n",
        "\n",
        "# Change the classifier head. In our case we have 2 categories \"dairy\" and \"others\".\n",
        "class_names = ['dairy', 'others']\n",
        "\n",
        "set_seeds()\n",
        "\n",
        "pretrained_vit.heads = nn.Linear(in_features=768, out_features=len(class_names)).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3feaa42",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Print a summary using torchinfo (uncomment for actual output)\n",
        "summary(model=pretrained_vit,\n",
        "        input_size=(32, 3, 224, 224), # (batch_size, color_channels, height, width)\n",
        "        # col_names=[\"input_size\"], # uncomment for smaller output\n",
        "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "        col_width=20,\n",
        "        row_settings=[\"var_names\"]\n",
        ")\n",
        "\n",
        "print(\"Notice how only the output layer is trainable, where as, all of the rest of the layers are untrainable (frozen).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset\n",
        "\n",
        "Image Classifier training expects the data to be in the format below. Divide the dataset into `train`, `valid` and `test` folders. Each category should have the labeling folder with their corresponding images. Folder names should be corresponding to the label names used while training.\n",
        "\n",
        "```\n",
        "dataset/\n",
        "├── train/\n",
        "│   ├── category_1\n",
        "                ├── Images\n",
        "│   ├── category_2\n",
        "                ├── Images\n",
        "├── valid/\n",
        "│   ├── Category_1\n",
        "                ├── Images\n",
        "│   ├── Category_2\n",
        "                ├── Images\n",
        "└── test/\n",
        "    ├── Category_1\n",
        "                ├── Images\n",
        "    ├── Category_2\n",
        "                ├── Images\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {},
      "id": "IwLOwR5q03YF"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "ac8cc699",
      "metadata": {
        "executionInfo": {
          "status": "ok",
          "timestamp": 1758836554345,
          "user_tz": 420,
          "elapsed": 9,
          "user": {
            "displayName": "Umair Sabir",
            "userId": "06940594206388957365"
          }
        }
      },
      "outputs": [],
      "source": [
        "# Setup directory paths to train and test images\n",
        "train_dir = '/content/Image-Classification-Using-Vision-transformer/train' # @param {type: \"string\", placeholder: \"[train_dir]\", isTemplate: true}\n",
        "valid_dir = '/content/Image-Classification-Using-Vision-transformer/val'    # @param {type: \"string\", placeholder: \"[valid_dir]\", isTemplate: true}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05aa777b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Remember, if you're going to use a pretrained model, it's generally important\n",
        "# to ensure your own custom data is transformed/formatted in the same way the\n",
        "# data the original model was trained on.\n",
        "# Get automatic transforms from pretrained ViT weights\n",
        "pretrained_vit_transforms = pretrained_vit_weights.transforms()\n",
        "print(pretrained_vit_transforms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9037c8a5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup dataloaders\n",
        "batch_size = 64\n",
        "train_dataloader_pretrained, test_dataloader_pretrained, class_names = create_dataloaders(train_dir=train_dir,\n",
        "                                                                                                     test_dir=valid_dir,\n",
        "                                                                                                     transform=pretrained_vit_transforms,\n",
        "                                                                                                     batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's visualize a image in order to know if data is loaded properly or not\n",
        "\n",
        "# Get a batch of images\n",
        "image_batch, label_batch = next(iter(train_dataloader_pretrained))\n",
        "\n",
        "# Get a single image from the batch\n",
        "image, label = image_batch[0], label_batch[0]\n",
        "\n",
        "# View the batch shapes.\n",
        "print(image.shape, label)\n",
        "\n",
        "plt.imshow(image.permute(1, 2, 0))\n",
        "plt.title(class_names[label])\n",
        "plt.axis(False);"
      ],
      "metadata": {},
      "id": "s-8PAlnlkU62",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_output_path = \"/mydrive/LLM/pet_grade_bottles/best_vit_model\" # @param {type: \"string\", placeholder: \"[model output]\", isTemplate: true}\n",
        "\n",
        "early_stopper = EarlyStopping(\n",
        "    patience=5,\n",
        "    delta=0.001,\n",
        "    verbose=True,\n",
        "    base_path=model_output_path\n",
        ")"
      ],
      "metadata": {},
      "id": "Y9ZKJoqIyjDE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create loss function.\n",
        "# OPTIONAL - Calculate weights to counteract class imbalance:\n",
        "# Formula: weight = total_samples / (num_classes * samples_per_class)\n",
        "# Example: weights = torch.tensor([42000 / (2 * 19000), 42000 / (2 * 23000)]).to(device)\n",
        "#          loss_fn = torch.nn.CrossEntropyLoss(weights=weights)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Create Optimizer and Scehduler.\n",
        "optimizer = torch.optim.AdamW(params=pretrained_vit.parameters(), lr=1e-5)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50, eta_min=1e-6)"
      ],
      "metadata": {},
      "id": "Cs8NzAKizK6N",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained_vit_results = training_with_callbacks.train(\n",
        "    model=pretrained_vit,\n",
        "    train_dataloader=train_dataloader_pretrained,\n",
        "    test_dataloader=test_dataloader_pretrained,\n",
        "    optimizer=optimizer,\n",
        "    loss_fn=loss_fn,\n",
        "    epochs=200,\n",
        "    device=device,\n",
        "    early_stopping=early_stopper,\n",
        "    scheduler=scheduler\n",
        ")\n",
        "\n",
        "# Plot the loss curves\n",
        "plot_loss_curves(pretrained_vit_results)"
      ],
      "metadata": {},
      "id": "HV5-8_k1xO1L",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
